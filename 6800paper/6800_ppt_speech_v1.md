# 6800 PPT 演讲稿（初版）

> 对应文件：`6800paper/6800_final_presentation_v5.pptx`  
> 建议总时长：10-12 分钟

## Slide 1 封面（约 30 秒）
大家好，我是 `<姓名/学号>`。今天汇报的题目是“基于借款描述文本的信用违约评估：传统模型与两阶段 LLM 的统一口径实验”。

这次项目的核心不是证明某个模型“绝对更强”，而是回答一个更实际的问题：在统一数据、统一流程和固定拒绝率约束下，什么模型更适合做主判，什么模型更适合做解释与复核辅助。

过渡：先快速看汇报结构。

## Slide 2 报告目录（约 40 秒）
这份汇报分成九个部分：研究动机、文献定位、数据与任务、方法协议、口径 A 主结果、扩展 LLM 与口径 B、稳健性分析、讨论与改进、可复现性与总结。

建议关注一个主线：固定拒绝率约束下的公平比较。因为在信贷业务里，模型精度并不是唯一目标，审批代价和拒绝规模必须一起看。

过渡：先说明研究问题和我们做了什么贡献。

## Slide 3 研究动机与核心问题（约 70 秒）
研究背景是典型的信息不对称：结构化变量不能完整表达借款人风险，而借款描述文本包含软信息，比如还款意愿、财务压力、事件冲击等。

我们提出三个核心问题：
1) 固定拒绝率下，文本模型是否稳定优于结构化模型？
2) 当拒绝率目标变化时，模型排序会不会迁移？
3) 两阶段 LLM 经过重标定后，能否带来真实业务收益？

对应贡献有三点：
- 第一，统一代码和评估口径，做真正可比的公平比较；
- 第二，采用共享子集与全量分布双口径，区分能力差异与分布差异；
- 第三，补充 RR 扫描、阈值敏感性和 Bootstrap 区间，提升结果可信度。

过渡：接下来把我们的文献定位讲清楚。

## Slide 4 文献脉络与本文定位（约 60 秒）
已有工作通常沿两条线：
- 一条是传统信用评分与结构化特征建模；
- 另一条是文本特征引入，包括主题模型、深度文本编码，再到近期 LLM。

本文的定位非常明确：
我们不试图证明“LLM 必然更好”，而是验证在同数据、同切分、同拒绝率约束下，哪类模型更适合作主判引擎，哪类模型更适合做解释辅助层。

换句话说，我们关注的是“可部署的比较结论”，不是单指标竞赛。

过渡：下面进入数据与任务设定。

## Slide 5 数据集、标签与双口径设置（约 70 秒）
标签定义很直接：`target=1` 对应 `Charged Off`，`target=0` 对应 `Fully Paid`。任务目标是：在固定拒绝率约束下识别违约并控制放行风险。

我们设置了双口径：
- 口径 A：共享子集 + RR=35%，用于公平横评；
- 口径 B：全量样本 + RR=0.1533，用于业务分布评估。

双口径的目的，是把“模型本身能力”和“样本分布/策略目标影响”分开看。这样后续如果口径 A 和口径 B 得出不同排序，我们能解释其原因，而不是把差异误判为模型波动。

过渡：有了数据设定后，下面看方法和评估协议。

## Slide 6 方法框架与评估协议（约 80 秒）
我们比较四类方法：DataAnalysis 规则化基线、传统 ML、BERT、以及 LLM 路线（two-stage/one-stage/external GPT-5.2）。

统一协议是本页重点：所有模型都遵循同样流程，在验证集按目标 RR 定阈，阈值固定后在测试集一次性评估，禁止测试集调参。

这套设计确保两件事：
- 第一，比较是公平的；
- 第二，结果可复现、可审计。

方法定位上，当前我们把结构化模型作为主判候选，把 BERT/LLM 主要评估为增益和可部署性，这个定位也和后面结论保持一致。

过渡：先看主榜结果，也就是口径 A。

## Slide 7 主结果（口径 A，RR=35%）（约 90 秒）
主榜规则是：只比较 Reject Rate 接近 35% 的模型。这样比较的是同等代价条件下的识别能力。

结果显示：
- `DataAnalysis` 的 Precision 最高，0.4175；
- `logistic_tabular` 的 Recall 更高，0.4538，同时 ABR 也更有竞争力。

所以在口径 A 下，最稳健的主判候选仍然是 DataAnalysis 与 logistic_tabular。两阶段 LLM 虽然可用，但在 Precision 和通过坏账率控制方面仍落后。

这页要带走的结论是：在固定拒绝率的公平条件下，结构化主线依然是当前最可靠方案。

过渡：接下来我们看扩展 LLM 和全量分布口径。

## Slide 8 扩展结果：LLM 对比与全量口径（约 90 秒）
这页分两部分。

第一部分是扩展 LLM 对比。One-stage 和 GPT-5.2 可能带来更高召回，但伴随明显 RR 偏移，因此不满足主榜入榜条件。这里的关键不是“数值高低”，而是“是否符合公平比较边界”。

第二部分是口径 B 全量分布。在 n=123,202、RR=0.1533 的设置下，`xgboost_tabular` 成为综合最优。

这意味着模型排序会随样本分布和目标 RR 改变。也就是说，口径 A 结论不能直接替代口径 B 结论。部署决策必须双口径并列汇报。

过渡：下面看稳健性证据，解释差距到底来自哪里。

## Slide 9 稳健性：拒绝率扫描与误差结构（约 75 秒）
稳健性分析关注两点：阈值变化下排名是否稳定，以及误差结构是否能解释业务后果。

这里有个非常关键的观察：`logistic_tabular` 和 `LLM Two-Stage` 的 FP 规模几乎相当，但前者每千笔多捕获约 30.2 笔违约。

这说明当前差距核心在违约识别能力（TP/FN 结构），而不是“谁拒得更多”。

这个发现对业务很重要，因为它告诉我们优化方向应放在识别能力和校准，而不是盲目抬高拒绝率。

过渡：基于这些结果，我们最后讨论局限和改进优先级。

## Slide 10 讨论、局限与改进优先级（约 80 秒）
为什么结构化主线仍占优？我们认为有五个原因：
- 高风险子集强化了结构化信号；
- 两阶段 LLM 教师理由仍有模板化问题；
- 决策头二元化，文本上下文利用不足；
- 0.6B 量级模型与当前数据规模存在错配；
- 无校准时容易出现“高拒绝率换高召回”的假优势。

有效性威胁也要明确：主实验聚焦高风险子集，外推需谨慎；时间外推验证还不完整；`desc` 字段历史可用性不均衡。

改进优先级建议：
- 高优先：教师信号升级 + 概率校准；
- 中优先：结构化与文本联合判别头；
- 低优先：只增加提示词复杂度。

过渡：下面用一页给出执行性更强的关键结论。

## Slide 11 关键结论单页（约 55 秒）
一句话总结是“三分法”：
- 口径 A 最优是 DataAnalysis / logistic_tabular；
- 口径 B 最优是 xgboost_tabular；
- LLM 当前定位是解释和复核辅助。

另外强调三条原则：
1) 高召回不等于更优，若 RR 偏离目标则不能进入主榜；
2) 当前差距核心在 TP/FN 结构，不是 FP 规模；
3) 下一步优先做教师信号、概率校准和联合判别头。

过渡：最后是可复现性和最终结论。

## Slide 12 可复现性与最终结论（约 80 秒）
我们把代码与结论做了明确映射：
- `workflow_common.py` 负责清洗、切分、阈值和指标；
- `ml_standalone/run_ml_pipeline.py` 负责传统 ML；
- `bert_standalone/run_bert_pipeline.py` 负责 BERT；
- `llm_standalone/run_llm_two_stage.py` 负责两阶段 LLM。

复现步骤也标准化为四步：先跑 ML/DataAnalysis/BERT，再跑 two-stage/one-stage LLM，再对齐评估外部 GPT 预测，最后更新 `6800main.md` 与最终文档。

最终结论是：固定拒绝率公平比较下，结构化模型仍是主判首选；两阶段 LLM 现阶段更适合作为解释和复核辅助层；下一步建议是“结构化主判 + LLM 解释 + 人工复核联动”的协同方案。

谢谢大家，欢迎提问。
