# 基于借款描述文本的信用违约评估：传统模型与两阶段 LLM 的统一口径实验

作者：`<姓名/学号>`  
课程：6800 Final Project  
日期：2026-02-21

## 摘要 (Abstract)
本文研究借款描述文本在信用违约预测中的增量价值。我们在统一代码与切分条件下，使用两套口径评估：共享高风险子集（`n=7108`，`RR=35%`）的公平横评，以及全量样本（`n=123,202`，`RR=0.1533`）的业务口径。口径 A 中，`DataAnalysis` 与 `logistic_tabular` 最稳健（`Precision@RR=0.4175/0.4154`），两阶段 LLM 为 `0.3653`；单阶段 LLM 与 GPT-5.2 虽召回更高，但拒绝率偏移明显（`0.6097/0.8228`），不满足同约束比较。口径 B 中，`xgboost_tabular` 综合最优（`Precision@RR=0.3153`, `Lift=2.0568`）。结论是：当前阶段结构化模型仍应作为主判，LLM 更适合作为解释与复核辅助层，并需先完成拒绝率校准。

**关键词 (Keywords)**: Credit Risk, LendingClub, Soft Information, P2P Lending, LLM, Fixed Reject Rate, Reproducibility

## I. 引言 (Introduction)
信用风险建模是零售信贷的核心任务。经典信用评分与机器学习方法已形成成熟范式 [1]-[4]，但在 P2P 借贷中，结构化字段难以完全覆盖借款人的真实风险，文本软信息因此具有补充价值 [6]-[10]。随着 Transformer/BERT/LoRA 等技术发展 [18]-[20]，文本建模能力提升明显，但其离线优势能否转化为可部署收益仍需在业务约束下验证。

本文关注固定拒绝率（Reject Rate）下的可复现实证：在同数据、同切分、同阈值流程下，文本模型是否稳定优于结构化基线。为避免单口径误判，本文同时报告共享子集 `RR=35%` 与全量口径 `RR=0.1533`。

本文贡献如下：  
1. 构建覆盖 `DataAnalysis/ML/BERT/LLM/GPT-5.2` 的统一评估框架。  
2. 提供双口径结果，并补充拒绝率扫描、阈值敏感性与 Bootstrap CI。  
3. 给出“结构化主判 + LLM 辅助”的可解释结论，并与代码产物一一对齐。  

## II. 主题描述与范围 (Topic Description and Scope)
### A. 综述主题与重要性
本文聚焦“借款文本软信息对违约预测的增量价值”。其意义在于：结构化字段存在信息盲区，而文本可补充还款意愿与风险语义；同时文本解释可直接服务人工复核。文献演进从结构化评分延伸到深度文本和 LLM [1]-[21]，因此本文强调“业务约束下的可比评估”，而非单纯离线分数比较。

### B. 研究问题（RQs）
1. `RQ1`：文本增强模型在统一口径下是否稳定优于结构化基线？  
2. `RQ2`：固定拒绝率约束是否改变模型排序？  
3. `RQ3`：两阶段 LLM 重标定能否带来可见业务收益？  
4. `RQ4`：LLM 在当前实现下更适合主判还是辅助解释？

### C. 不在本文范围 (Out of Scope)
本文不展开反欺诈任务、贷后催收优化、宏观压力测试、多模态/隐私计算实现等议题，以保持“违约预测 + 固定拒绝率评估”主线聚焦。

## III. 文献综述与综合分析 (Literature Review and Synthesis)
### A. 经典信用评分与机器学习基线
经典研究建立了信用评分与可比较评估范式 [1]-[4]，XGBoost 等方法推动了工程化应用 [5]。这一阶段对本文的启发是：比较必须在同样本、同指标、同验证流程下进行。

### B. P2P 借贷中的信息不对称与软信息
P2P 场景的信息不对称更突出。相关研究证明社交关系与文本软信息可补充传统结构化因子 [6]-[10]，研究重点由“纯硬信息评分”转向“硬信息 + 软信息”联合建模。

### C. 文本软信息建模：从特征工程到深度语义
文本建模从词频/主题特征逐步发展到深度语义表示，普遍报告了文本增益 [11]-[16]。但增益强弱受样本规模、文本质量和评估口径影响，在固定阈值与运营稳定性约束下，离线提升不一定等价于上线收益。

### D. LLM 阶段：能力扩展与落地约束
LLM 提升了语义理解与解释生成能力 [18]-[21]，并开始进入金融风险管理研究 [17], [22]。但落地仍受三类约束：拒绝率可控性、误差结构可接受性、教师信号质量。

### E. 文献综合：阶段演进与局限传递
| 阶段 | 代表文献 | 主要贡献 | 典型局限 | 后续阶段如何改进 |
|---|---|---|---|---|
| 经典评分 | [1], [2], [3], [4] | 建立信用评分评估范式与基线 | 对软信息覆盖不足 | 引入平台行为与软信息 |
| P2P 实证 | [6], [7], [8], [9], [10] | 证明信息不对称与软筛选存在 | 文本利用深度不足 | 文本语义建模与融合 |
| 文本特征工程 | [11], [12], [13], [14] | 量化文本增量价值 | 语义表达能力受限 | 深度文本与上下文建模 |
| 深度文本 | [15], [16] | 提升上下文建模与泛化能力 | 解释与阈值控制仍困难 | LLM + 参数高效微调 |
| LLM 应用 | [17], [18], [19], [20], [21] | 强语义理解与解释生成 | 业务稳定性与校准难题 | 强化校准、蒸馏与人机协同 |

### F. 方法比较：为何同为“文本模型”结论仍不一致
文本收益结论不一致的主要原因通常是任务定义与评估协议差异，而非模型“代际差异”。为避免跨口径误导，本文将关键差异归纳为四类：

| 比较维度 | 常见做法差异 | 对结论的影响 |
|---|---|---|
| 标签策略 | 是否只保留 Fully Paid / Charged Off，是否混入进行中贷款 | 混入未终局样本会引入标签噪声，夸大或掩盖文本贡献 |
| 评估口径 | AUC/PR-AUC vs 固定拒绝率业务指标 | 仅看排序指标可能高估上线可用性 |
| 文本粒度 | 标题短文本、描述长文本、二者融合 | 长文本更富信息，但训练难度与噪声控制要求更高 |
| 特征融合 | 早期拼接 vs 联合训练 vs 两阶段决策 | 融合方式决定结构化强信号是否被文本有效补充 |

既有研究多基于离线分类指标 [10], [11], [16]；当评估切换到固定拒绝率与放行坏账率时，排序可能迁移。本文即围绕这一“指标迁移 -> 结论迁移”现象做统一实证。

### G. 本文定位
基于上述脉络，本文不试图证明“LLM 必然优于传统模型”，而是回答更实际的问题：在统一业务口径（固定拒绝率）下，哪类模型更适合作为主判，哪类模型更适合作为解释层。

### H. 文献缺口与实验设计映射
为避免综述与实验脱节，本文将文献缺口直接映射到实验：固定 RR 公平主榜、全量口径外推、误差结构代价解释、拒绝率扫描与 Bootstrap CI。这样每条结论都可回溯到对应文献动机与实验协议。

## IV. 数据集、任务与特征工程 (Dataset, Task, and Feature Engineering)
### A. 数据来源与样本构建
原始数据来自 LendingClub 公开历史数据 [23]，并通过 `workflow_common.py` 与各流水线脚本完成统一清洗与共享切分。

| 阶段 | 样本量 | 说明 |
|---|---:|---|
| 原始读取 | 2,260,701 | `accepted_2007_to_2018Q4.csv` |
| 初筛后 | 123,293 | 满足基础状态与字段条件 |
| 清洗后 | 123,202 | 去重/缺失清洗后可用样本 |
| 共享高风险子集 | 7,108 | `grade in {E,F,G}`, `int_rate>=18.5`, `annual_inc<=92,500` |

### B. 标签定义与任务
- `target=1`: Charged Off（违约）  
- `target=0`: Fully Paid（履约）  

任务是违约二分类；评估目标是固定拒绝率口径下的识别质量，而非仅比较 AUC。

### C. 子集分布与难度特征
共享子集违约率约 `35.00%`，且集中在较高风险区间。该设定会增强结构化信号（利率、等级、DTI 等），并压缩文本增益的边际空间。

| 维度 | 统计 |
|---|---|
| 样本总数 | 7,108 |
| 违约率 | 35.00% |
| 年份范围 | 2008-2016 |
| Grade 分布 | E: 4,414 / F: 2,195 / G: 499 |
| Grade 违约率 | E: 32.85% / F: 37.63% / G: 42.48% |

### D. 共享切分与防泄露控制（口径 A）
统一采用分层随机切分（`data/shared/splits/stratified`）：

- `train_fit=4548`  
- `validation=1138`  
- `test=1422`  

并通过 `_shared_row_id` 对齐各流水线样本，确保测试集完全一致。

### E. 全量切分与业务分布口径（口径 B）
为评估真实分布下的表现，本文补充口径 B：

- 输入：`ml_standalone/data/processed/ml_full_processed.csv`（`n=123,202`）  
- 共享切分目录：`data/shared/splits/full_actual_rr`  
- 切分规模：`train_fit=78848`, `validation=19713`, `test=24641`  
- 目标拒绝率：`RR_target=0.15327673252057597`（全量真实违约率）  

该口径当前覆盖 `ML + DataAnalysis`，用于检验主模型排序在全量分布下是否迁移。

### F. 特征体系
| 特征组 | 代表字段 | 在本项目中的作用 |
|---|---|---|
| 数值特征 | `loan_amnt`, `int_rate`, `annual_inc`, `dti`, `fico_mean` | 主风险强信号 |
| 类别特征 | `grade`, `home_ownership`, `purpose`, `verification_status` | 结构化分群 |
| 文本特征 | `desc_clean` / `bert_input` | 软信息与语义补充 |
| 流程特征 | `_shared_row_id`, `issue_year` | 对齐切分与误差诊断 |

## V. 方法 (Methods)
### A. 统一方法框架
本文比较四类方法族：

1. `DataAnalysis` 风险画像分类器（规则化分数与阈值策略）。  
2. 传统 ML（逻辑回归、XGBoost）。  
3. BERT 路线（Embedding + 逻辑回归；端到端微调）。  
4. LLM 路线（两阶段微调、单阶段微调、外部 GPT-5.2 推理）。  

所有方法均遵循“验证集定阈值、测试集仅评估”，不使用测试集调参。`DataAnalysis` 作为可解释、低复杂度的业务基线。

### B. 传统 ML 基线
对应代码：`ml_standalone/run_ml_pipeline.py`。

- 预处理：数值 `median+scaler`，类别 `most_frequent+one-hot`，文本融合 `TF-IDF(1-2gram, min_df=5, max_features=30000)`。  
- 候选模型：
  - `logistic_tabular`（`max_iter=1800`, `class_weight=balanced`）  
  - `logistic_text_fusion`（结构化 + TF-IDF）  
  - `xgboost_tabular`（`n_estimators=260`, `max_depth=6`, `learning_rate=0.05` 等）

ML 使用 `cv_folds=3` 分层交叉验证做训练参考，最终仍以验证集固定 RR 指标筛选。结果显示 `logistic_text_fusion` 未显著超过 `logistic_tabular`，说明当前子集中文本边际增益受样本与信号结构限制。

### C. BERT 路线
对应代码：`bert_standalone/run_bert_pipeline.py`, `bert_standalone/bert_workflow.py`。

1. **Embedding 路线**：`bce-embedding-base_v1` 编码 `bert_input`，再接逻辑回归分类器。  
2. **Finetune 路线**：同底座模型端到端微调（`learning_rate=2e-5`, `train_batch_size=8`, `num_train_epochs=1.0`）。  
3. 输入长度：请求 `max_length=514`，实际按 tokenizer 限制使用 `512`。  

### D. LLM 路线（两阶段/单阶段/外部）
对应代码：`llm_standalone/run_llm_two_stage.py`、`llm_standalone/run_llm_one_stage.py`，一键脚本：`run_llm_two_stage_full_06b.ps1` 与 `run_llm_one_stage_full_06b.ps1`。

流程包括两阶段（`Input->Reason->Action`）、单阶段（`Input->Action`）和外部 GPT-5.2 对齐评估。训练基于 `Qwen3-0.6B + LoRA(rank=8)`，推理时用 `reject/approve` 的 logprob 计算拒绝概率。两阶段支持验证集重标定；单阶段与 GPT-5.2 在共享口径下存在明显 RR 偏移，因此仅作扩展比较。

### E. 指标定义与业务意义
以“拒绝”为正类，本文在两套目标拒绝率下评估：共享子集 `RR_target=0.35`，全量口径 `RR_target=0.15327673252057597`。

- `Precision@RR = TP / (TP + FP)`：被拒样本中真实违约占比。  
- `Recall@RR = TP / (TP + FN)`：违约捕获率。  
- `Approval Bad Rate = FN / (TN + FN)`：放行样本中的坏账率。  
- `Lift@RR = Precision@RR / RR_target`：相对随机拒绝的提升倍数（口径内比较）。  

这些指标可直接映射“误拒/误放”权衡，比单一 AUC 更贴近审批流程；不同 `RR_target` 下的绝对值不应横向比较。

### F. 统一阈值选择流程（防止口径漂移）
1. 在验证集上按目标拒绝率寻找分位点阈值（或等价概率阈值）。  
2. 记录该阈值下的 `Precision@RR`、`Recall@RR`、`Approval Bad Rate`。  
3. 固定阈值后一次性在测试集评估，不再回看测试结果调阈。  
4. 对 LLM 额外报告“默认阈值 vs 固定 RR 阈值”的敏感性，以分离“模型能力”与“阈值校准”影响。  

## VI. 实验设计与统计协议 (Experiment Design and Statistical Protocol)
### A. 公平比较控制
- 口径 A（公平横评）：`data/shared/shared_subset.csv` + `data/shared/splits/stratified` + `RR=35%`  
- 口径 B（业务分布）：`ml_full_processed.csv` + `data/shared/splits/full_actual_rr` + `RR=0.1533`（精确值见 `IV.E`）  
- 同一阈值流程：验证集定阈 -> 测试集固定评估  
- 对拒绝率显著偏离目标的模型单独标注（不并入公平主榜）  

### B. 实验阶段
1. **主对比实验（口径 A）**：7 条对齐路线（DA/3个ML/BERT-Emb/BERT-FT/LLM-2stage）。  
2. **扩展实验（口径 A）**：LLM 单阶段与 GPT-5.2（报告但不纳入公平主榜）。  
3. **拒绝率扫描实验**：在 `10%/20%/35%/50%` 口径比较模型排序变化。  
4. **LLM 阈值敏感性**：对比默认阈值与固定 RR 重标定。  
5. **不确定性评估**：对关键模型做 bootstrap 95% CI（`B=1000`）。  
6. **全量口径实验（口径 B）**：ML + DataAnalysis 在实际违约率目标下比较。

### C. 实证规范说明（与课程最佳实践对齐）
核心规范：无测试集调参；交叉验证仅用于训练参考；阈值统一由验证集业务目标确定；所有模型采用同一业务指标解释。  

### D. 统计与稳健性设计
稳健性检查包括两部分：  
1. **拒绝率扫描**：在 `10%/20%/35%/50%` 下比较排序是否迁移。  
2. **Bootstrap CI**：对关键模型做 `B=1000` 次重采样并报告 95% CI。  

### E. 模型入榜判定规则（执行口径）
1. 先检验是否满足目标拒绝率约束（或在可接受偏差区间内）。  
2. 仅对满足约束的模型比较 `Precision@RR`、`Lift@RR` 与 `Approval Bad Rate`。  
3. 对拒绝率显著偏离目标的模型仅保留在扩展实验，不并入公平主榜。

若主指标接近，优先选择阈值更稳定、解释成本更低的方案。

## VII. 实验结果 (Experimental Results)
### A. 主结果（口径 A：共享子集，RR 目标=35%，公平主榜）
| 模型 | Precision@RR | Recall@RR | Approval Bad Rate | Lift@RR | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| DataAnalysis | 0.4175 | 0.4317 | 0.3120 | 1.1928 | 0.3622 |
| ML: logistic_tabular | 0.4154 | 0.4538 | 0.3098 | 1.1870 | 0.3826 |
| ML: logistic_text_fusion | 0.4154 | 0.4337 | 0.3126 | 1.1868 | 0.3657 |
| ML: xgboost_tabular | 0.4030 | 0.4378 | 0.3178 | 1.1513 | 0.3805 |
| BERT-Embedding | 0.3859 | 0.4177 | 0.3284 | 1.1026 | 0.3790 |
| BERT-Finetune | 0.3895 | 0.4317 | 0.3253 | 1.1128 | 0.3882 |
| LLM Two-Stage (Qwen3-0.6B) | 0.3653 | 0.3675 | 0.3420 | 1.0436 | 0.3523 |

在拒绝率接近 35% 的公平约束下，`DataAnalysis` 与 `logistic_tabular` 仍最稳健；LLM 两阶段可用但与结构化主线存在明显差距。

### B. 扩展 LLM 对比（口径 A：同测试集，但未全部满足 RR 对齐）
| 模型 | Precision@RR | Recall@RR | Approval Bad Rate | Lift@RR | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| LLM Two-Stage (Qwen3-0.6B) | 0.3653 | 0.3675 | 0.3420 | 1.0436 | 0.3523 |
| LLM One-Stage (Qwen3-0.6B) | 0.3587 | 0.6245 | 0.3369 | 1.0249 | 0.6097 |
| GPT-5.2 (External, no finetune) | 0.3462 | 0.8133 | 0.3690 | 0.9884 | 0.8228 |

扩展结果表明：单阶段 LLM 与 GPT-5.2 的高召回主要由高拒绝率驱动，因此不并入主榜结论。

### C. 全量业务口径结果（口径 B：n=123,202，RR 目标=0.1533）
| 模型 | Precision@RR | Recall@RR | Approval Bad Rate | Lift@RR | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML: logistic_tabular | 0.3069 | 0.2992 | 0.1263 | 2.0023 | 0.1494 |
| ML: logistic_text_fusion | 0.2994 | 0.3050 | 0.1262 | 1.9532 | 0.1562 |
| ML: xgboost_tabular | 0.3153 | 0.3095 | 0.1246 | 2.0568 | 0.1505 |
| DataAnalysis | 0.2849 | 0.2851 | 0.1294 | 1.8589 | 0.1534 |

在全量分布下，`xgboost_tabular` 取代 `logistic_tabular` 成为最优，说明排序受分布与 `RR_target` 影响。

### D. 拒绝率扫描：模型排序是否稳定（口径 A）
| 模型 | RR=10% Precision | RR=20% Precision | RR=35% Precision | RR=50% Precision |
|---|---:|---:|---:|---:|
| ML: logistic_tabular | 0.5180 | 0.4712 | 0.4154 | 0.4023 |
| DataAnalysis | 0.4797 | 0.4254 | 0.4175 | 0.3992 |
| ML: xgboost_tabular | 0.4248 | 0.4415 | 0.4030 | 0.3963 |
| BERT-Finetune | 0.4044 | 0.3789 | 0.3895 | 0.3826 |

低拒绝率（10%-20%）下 `logistic_tabular` 优势更明显；拒绝率上升后差距缩小，但结构化路线仍领先。

### E. 两阶段 LLM 阈值敏感性
| 设置 | Reject Rate | Precision | Recall | Approval Bad Rate | Lift |
|---|---:|---:|---:|---:|---:|
| 默认阈值（delta>=0） | 0.2820 | 0.3691 | 0.2972 | 0.3428 | 1.0545 |
| 固定 RR=35%（验证集重标定） | 0.3523 | 0.3653 | 0.3675 | 0.3420 | 1.0436 |

重标定主要改善召回（+7.03 个百分点），但 Precision 略降，放行坏账率改善有限。

### F. 不确定性（Bootstrap 95% CI）
| 模型 | Precision (95% CI) | Recall (95% CI) | Approval Bad Rate (95% CI) |
|---|---|---|---|
| DataAnalysis | 0.4175 ([0.3735, 0.4605]) | 0.4317 ([0.3906, 0.4767]) | 0.3120 ([0.2834, 0.3428]) |
| ML: logistic_tabular | 0.4154 ([0.3766, 0.4532]) | 0.4538 ([0.4094, 0.5000]) | 0.3098 ([0.2793, 0.3395]) |
| BERT-Finetune | 0.3895 ([0.3498, 0.4328]) | 0.4317 ([0.3861, 0.4807]) | 0.3253 ([0.2957, 0.3573]) |
| LLM Two-Stage FixedRR | 0.3653 ([0.3245, 0.4077]) | 0.3675 ([0.3273, 0.4088]) | 0.3420 ([0.3131, 0.3706]) |

置信区间与主结论一致：LLM 与结构化基线在 Precision 和 Approval Bad Rate 上仍有差距。

### G. 误差结构视角：每千笔申请的业务代价（口径 A）
为增强业务可读性，本文将主结果换算为“每 1000 笔申请”的误差结构：

| 模型 | 每1000笔捕获违约（TP） | 每1000笔放过违约（FN） | 每1000笔误拒好人（FP） |
|---|---:|---:|---:|
| ML: logistic_tabular | 158.9 | 191.3 | 223.6 |
| DataAnalysis | 151.2 | 199.0 | 211.0 |
| BERT-Finetune | 151.2 | 199.0 | 237.0 |
| LLM Two-Stage (Qwen3-0.6B) | 128.7 | 221.5 | 223.6 |

`logistic_tabular` 与两阶段 LLM 的误拒规模几乎相同（`223.6` vs `223.6`），但前者每 1000 笔多捕获约 `30.2` 笔违约，说明差距主要来自识别能力而非过度拒绝。

## VIII. 讨论与分析 (Discussion and Analysis)
### A. 为什么当前结构化模型仍占优
1. **样本选择效应**：共享子集通过高风险规则筛选，结构化信号浓度提升。  
2. **训练信号约束**：两阶段 LLM 的教师理由来自规则化模板，语义多样性不足。  
3. **决策头限制**：Stage-2 最终为二元动作比较，难充分利用复杂上下文。  
4. **模型规模与样本规模错配**：`0.6B` 在当前数据量下更易表现为“可解释但判别力有限”。  
5. **校准约束差异**：单阶段 LLM 与 GPT-5.2 在当前配置下表现为“高拒绝率换高召回”，难直接满足固定 RR 运营规则。  

### B. 与文献脉络的对应关系
文献普遍显示文本有增量价值 [10]-[17]，但增益依赖任务定义与评估口径。本文结果不否定文本价值，而是表明在固定拒绝率约束下，文本增益需依赖更强训练与校准；且全量分布下结构化模型内部排序也会变化（`xgboost_tabular` 领先）。

### C. 哪些方向更有前景
1. **更高质量教师信号**：从规则模板升级到专家审阅理由，减少模式坍塌。  
2. **联合建模**：将结构化强信号与文本语义在同一判别头融合，而非简单串联。  
3. **校准优先**：引入温度缩放/分段校准与 top-k 约束，优先优化固定 RR 下的业务指标。  
4. **时序外推验证**：补充时间外推切分，验证跨年份稳定性。  

### D. 哪些方向前景有限
1. 仅靠更复杂提示词而不改变训练信号质量，边际收益通常有限。  
2. 在无校准前提下直接将 LLM 作为主判，会放大运营波动与审核负担。  
3. 将“高拒绝率高召回”结果直接宣称为更优策略，会忽略业务约束与客户体验成本。  

### E. 威胁与有效性（Threats to Validity）
1. **样本代表性限制**：主实验聚焦高风险子集，结论不直接代表全量客群；但该设置有助于回答“高风险审批”场景问题。  
2. **时序外推未完全验证**：当前以分层随机切分为主，尚未进行严格按时间滚动测试；因此跨周期稳定性仍需补充实验。  
3. **LLM 教师信号偏差**：两阶段数据由教师流程构造，若理由文本模式较单一，可能限制学生模型泛化。  
4. **文本字段历史缺失**：`desc` 在历史期间存在可用性差异，这会影响时间段之间的可比性。  

## IX. 代码组织与可复现性 (Code Organization and Reproducibility)
### A. 代码结构与职责映射
| 目录/脚本 | 职责 |
|---|---|
| `workflow_common.py` | 清洗、切分、阈值优化、统一指标函数 |
| `ml_standalone/run_ml_pipeline.py` | 传统 ML 训练、验证定阈、测试评估 |
| `data_analysis_standalone/run_data_analysis_classifier.py` | 数据分析规则化风险评分基线 |
| `bert_standalone/run_bert_pipeline.py` | BERT embedding/finetune 训练与评估 |
| `llm_standalone/run_llm_two_stage.py` | 两阶段 LLM 数据构建、训练配置、推理评估 |
| `llm_standalone/run_llm_one_stage.py` | 单阶段 LLM 训练与推理评估 |
| `llm_standalone/run_llm_two_stage_full_06b.ps1` | 两阶段 LLM 一键流程脚本 |
| `llm_standalone/run_llm_one_stage_full_06b.ps1` | 单阶段 LLM 一键流程脚本 |
| `6800paper/scripts/md_to_ieee_docx.py` | 从 `6800main.md` 自动生成 IEEE 双栏 Word |

### B. 一致性与复现实证
关键结论可由三类文件复核：主结果表（`output/doc/report_tables/*.csv`）、模型报告（`output/*/run_report.json`）、LLM/GPT 推理输出（`output/llm_*/*json`）。

### C. 复现步骤（建议写入提交说明）
1. 重建共享切分并运行 `ML + DataAnalysis + BERT`。  
2. 运行 `run_llm_two_stage_full_06b.ps1` 与 `run_llm_one_stage_full_06b.ps1`。  
3. 运行外部预测评估脚本并更新 `gpt52_external_metrics.json`。  
4. 运行全量口径 `ML + DataAnalysis`。  
5. 更新 `6800main.md` 后执行 `md_to_ieee_docx.py` 生成最终 IEEE 文档。  

## X. 结论 (Conclusion)
本文在统一数据、切分与阈值流程下，对传统模型、BERT、两阶段/单阶段 LLM 与外部 GPT-5.2 进行了双口径比较。结论如下：

1. 结构化模型（尤其 `logistic_tabular` 与 `DataAnalysis`）在固定拒绝率场景下仍更稳健。  
2. 两阶段 LLM 的阈值重标定可以显著提升召回，但不足以弥补整体判别差距；单阶段 LLM 与 GPT-5.2 当前主要通过高拒绝率获取高召回。  
3. 全量真实分布口径下 `xgboost_tabular` 综合最优，说明模型排序受样本分布与目标 RR 影响。  
4. 当前最可行路线是“结构化主判 + LLM 解释辅助 + 人工复核联动”。  

本文的主要价值是建立与业务约束一致、与代码产物一致、可复现可审计的比较框架，而非追求单一离线高分。

## 参考文献 (References)
[1] D. J. Hand and W. E. Henley, “Statistical Classification Methods in Consumer Credit Scoring: A Review,” *Journal of the Royal Statistical Society Series A: Statistics in Society*, vol. 160, no. 3, pp. 523-541, 1997, doi: 10.1111/j.1467-985X.1997.00078.x.  
[2] L. C. Thomas, “A survey of credit and behavioural scoring: forecasting financial risk of lending to consumers,” *International Journal of Forecasting*, vol. 16, no. 2, pp. 149-172, 2000, doi: 10.1016/S0169-2070(00)00034-0.  
[3] A. E. Khandani, A. J. Kim, and A. W. Lo, “Consumer credit-risk models via machine-learning algorithms,” *Journal of Banking & Finance*, vol. 34, no. 11, pp. 2767-2787, 2010, doi: 10.1016/j.jbankfin.2010.06.001.  
[4] S. Lessmann, B. Baesens, H.-V. Seow, and L. C. Thomas, “Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research,” *European Journal of Operational Research*, vol. 247, no. 1, pp. 124-136, 2015, doi: 10.1016/j.ejor.2015.05.030.  
[5] T. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2016, pp. 785-794, doi: 10.1145/2939672.2939785.  
[6] M. Lin, N. R. Prabhala, and S. Viswanathan, “Judging Borrowers by the Company They Keep: Friendship Networks and Information Asymmetry in Online Peer-to-Peer Lending,” *Management Science*, vol. 59, no. 1, pp. 17-35, 2013, doi: 10.1287/mnsc.1120.1560.  
[7] C. Serrano-Cinca, B. Gutiérrez-Nieto, and L. López-Palacios, “Determinants of Default in P2P Lending,” *PLOS ONE*, vol. 10, no. 10, p. e0139427, 2015, doi: 10.1371/journal.pone.0139427.  
[8] R. Emekter, Y. Tu, B. Jirasakuldech, and M. Lu, “Evaluating credit risk and loan performance in online Peer-to-Peer (P2P) lending,” *Applied Economics*, vol. 47, no. 1, pp. 54-70, 2015, doi: 10.1080/00036846.2014.962222.  
[9] R. Iyer, A. I. Khwaja, E. F. P. Luttmer, and K. Shue, “Screening Peers Softly: Inferring the Quality of Small Borrowers,” *Management Science*, vol. 62, no. 6, pp. 1554-1577, 2016, doi: 10.1287/mnsc.2015.2181.  
[10] G. Dorfleitner, C. Priberny, S. Schuster, J. Stoiber, M. Weber, I. de Castro, and J. Kammler, “Description-text related soft information in peer-to-peer lending - Evidence from two leading European platforms,” *Journal of Banking & Finance*, vol. 64, pp. 169-187, 2016, doi: 10.1016/j.jbankfin.2015.11.009.  
[11] C. Jiang, Z. Wang, R. Wang, and Y. Ding, “Loan default prediction by combining soft information extracted from descriptive text in online peer-to-peer lending,” *Annals of Operations Research*, vol. 266, no. 1-2, pp. 511-529, 2018, doi: 10.1007/s10479-017-2668-z.  
[12] J. Yao, J. Chen, J. Wei, Y. Chen, and S. Yang, “The relationship between soft information in loan titles and online peer-to-peer lending: evidence from RenRenDai platform,” *Electronic Commerce Research*, vol. 19, no. 1, pp. 111-129, 2019, doi: 10.1007/s10660-018-9293-z.  
[13] C. Wang, W. Zhang, X. Zhao, and J. Wang, “Soft information in online peer-to-peer lending: Evidence from a leading platform in China,” *Electronic Commerce Research and Applications*, vol. 36, p. 100873, 2019, doi: 10.1016/j.elerap.2019.100873.  
[14] Y. Xia, L. He, Y. Li, N. Liu, and Y. Ding, “Predicting loan default in peer-to-peer lending using narrative data,” *Journal of Forecasting*, vol. 39, no. 2, pp. 260-280, 2020, doi: 10.1002/for.2625.  
[15] Z. Wang, C. Jiang, H. Zhao, and Y. Ding, “Mining Semantic Soft Factors for Credit Risk Evaluation in Peer-to-Peer Lending,” *Journal of Management Information Systems*, vol. 37, no. 1, pp. 282-308, 2020, doi: 10.1080/07421222.2019.1705513.  
[16] J. Kriebel and L. Stitz, “Credit default prediction from user-generated text in peer-to-peer lending using deep learning,” *European Journal of Operational Research*, vol. 302, no. 1, pp. 309-323, 2022, doi: 10.1016/j.ejor.2021.12.024.  
[17] M. Siering, “Peer-to-Peer (P2P) Lending Risk Management: Assessing Credit Risk on Social Lending Platforms Using Textual Factors,” *ACM Transactions on Management Information Systems*, vol. 14, no. 3, pp. 1-19, 2023, doi: 10.1145/3589003.  
[18] A. Vaswani *et al*., “Attention Is All You Need,” in *Advances in Neural Information Processing Systems 30*, 2017, doi: 10.48550/arXiv.1706.03762.  
[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” in *Proceedings of NAACL-HLT*, 2019, pp. 4171-4186, doi: 10.18653/v1/N19-1423.  
[20] E. J. Hu *et al*., “LoRA: Low-Rank Adaptation of Large Language Models,” arXiv:2106.09685, 2021, doi: 10.48550/arXiv.2106.09685.  
[21] OpenAI *et al*., “GPT-4 Technical Report,” arXiv:2303.08774, 2023, doi: 10.48550/arXiv.2303.08774.  
[22] T. Berg, V. Burg, A. Gombović, and M. Puri, “On the Rise of FinTechs: Credit Scoring Using Digital Footprints,” *The Review of Financial Studies*, vol. 33, no. 7, pp. 2845-2897, 2020, doi: 10.1093/rfs/hhz099.  
[23] LendingClub, “LendingClub Statistics and Loan Data,” [Online]. Available: https://www.lendingclub.com/info/download-data.action (archived: https://web.archive.org/web/20110816083005/https://www.lendingclub.com/info/download-data.action). [Accessed: Feb. 21, 2026].
