# 基于借款描述文本的信用违约评估：传统模型与两阶段 LLM 的统一口径实验

作者：`<姓名/学号>`  
课程：6800 Final Project  
日期：2026-02-21

## Abstract
在网络借贷风控场景中，结构化变量（如利率、信用等级、收入）具有稳定预测能力，但对借款人风险的刻画并不完整。借款文本（例如 LendingClub 的 `desc` 字段）承载了还款意愿、财务压力、用途可信度等软信息，因此成为信息不完全条件下的重要补充。本文在统一代码框架下采用两套评估口径：其一是共享高风险子集（`n=7108`）上的固定拒绝率 `RR=35%` 公平横评，其二是全量样本（`n=123,202`）上的实际违约率目标 `RR=0.1533`。共享子集结果显示，在拒绝率接近 35% 的模型中，`DataAnalysis` 与 `logistic_tabular` 表现最佳（`Precision@RR` 分别为 `0.4175` 和 `0.4154`），两阶段 LLM 为 `0.3653`。扩展比较中，单阶段 LLM 与 GPT-5.2 虽获得更高召回（分别 `0.6245` 与 `0.8133`），但实际拒绝率显著偏高（`0.6097` 与 `0.8228`），不满足同约束公平比较。全量口径下，`xgboost_tabular` 综合最优（`Precision@RR=0.3153`, `Lift=2.0568`）。结论是：当前训练规模与约束下，结构化模型仍应作为主判引擎，LLM 更适合作为解释与复核辅助层，并需先完成拒绝率校准再参与主榜对比。

**Keywords**: Credit Risk, LendingClub, Soft Information, P2P Lending, LLM, Fixed Reject Rate, Reproducibility

## I. Introduction
信用风险建模长期是零售信贷与消费金融的核心问题。经典研究已经系统总结了信用评分中的统计分类方法与评估框架 [1], [2]，并在后续实践中扩展到机器学习方法 [3], [4]。在平台型借贷（P2P）环境中，信息不对称更突出，借款描述、社交关系和文本表达方式等软信息对风险识别具有现实价值 [6]-[10]。

随着深度学习与大语言模型的发展，文本建模能力显著增强：Transformer 与 BERT 改善了语义表示 [18], [19]，LoRA 等参数高效微调降低了大模型落地门槛 [20]。但“语言能力强”并不自动等价于“业务决策优”，尤其在真实风控约束下，模型必须同时满足阈值可控、误拒误放可解释、跨批次稳定等条件。

本项目聚焦一个可操作问题：在拒绝率约束（Reject Rate）下，文本模型是否能够带来可观、可复现的增益？我们并不追求“绝对准确率最高”的离线对比，而是强调“同口径、同数据、同切分、同阈值策略”下的公平比较。为避免单一口径偏差，本文同时报告共享子集 `RR=35%` 公平横评和全量样本实际违约率口径 `RR=0.1533` 的结果。

本文贡献如下：

1. 提供覆盖传统模型、BERT、两阶段/单阶段 LLM 与外部 GPT-5.2 的统一评估框架，并严格对齐数据与切分。  
2. 在双口径（共享子集 `RR=35%` 与全量实际违约率 `RR=0.1533`）下报告主指标与业务指标，并补充拒绝率扫描、阈值敏感性与置信区间分析。  
3. 给出文献脉络驱动的实证解释：为何结构化模型在当前设置下仍占优，以及 LLM 在风控流程中的合理定位。  
4. 将论文描述与仓库代码路径逐项对齐，提升“报告-代码一致性”和可复现性。  

## II. Topic Description and Scope
### A. 综述主题与重要性
本文的综述主题是“借款文本软信息在违约预测中的增量价值”。该主题的重要性体现在两点：

1. 信息不完全：放贷方常仅能获得有限结构化字段，难覆盖借款人的真实偿付能力与行为风险。  
2. 决策可解释性：文本可用于生成风险理由和审核摘要，直接服务于人工复核流程。  

从文献演进看，该主题经历了“结构化评分 -> P2P 信息不对称 -> 文本软信息 -> 深度文本 -> LLM 辅助决策”的连续发展链条 [1]-[21]。因此，单纯比较某个模型的分数并不足够，必须将实验放回文献脉络中解释其合理性。

### B. 研究问题（RQs）
本文围绕以下四个研究问题：

1. `RQ1`：在统一口径下，文本增强模型是否稳定优于结构化基线？  
2. `RQ2`：固定拒绝率约束是否改变模型间相对排序？  
3. `RQ3`：两阶段 LLM 的阈值重标定能否显著改善业务指标？  
4. `RQ4`：在当前工程实现下，LLM 更适合“主判”还是“辅助解释”？  

### C. Out of Scope（明确不覆盖）
为保证综述与实验聚焦，本文不展开以下方向：

1. 反欺诈与身份欺诈检测（目标函数与违约预测不同）。  
2. 贷后催收策略与动态干预优化。  
3. 宏观情景压力测试与资产定价联动模型。  
4. 多模态风控（设备指纹、图结构、社交图）与隐私计算实现细节。  

## III. Literature Review and Synthesis
### A. 经典信用评分与机器学习基线
Hand 与 Henley 的综述是信用评分方法学的重要起点 [1]，Thomas 则系统总结了行为评分与风险预测实践 [2]。此后，机器学习在消费信贷中的应用逐步成熟，例如 Khandani 等基于大样本验证了 ML 的可行性 [3]。Lessmann 等给出了信用评分分类器的系统基准比较 [4]，而 XGBoost 为后续树模型工程化提供了通用框架 [5]。

这一阶段的核心贡献是：明确了“可比较评估协议”的重要性（同样本、同指标、同验证流程）。本文沿用了这一思想，避免跨数据或跨阈值口径比较造成的结论偏差。

### B. P2P 借贷中的信息不对称与软信息
在 P2P 借贷中，信息不对称更强。Lin 等指出社交网络关系可缓解信息不对称 [6]；Serrano-Cinca 与 Emekter 分别从平台数据实证了结构化风险因子的预测作用 [7], [8]；Iyer 等进一步强调了市场中的“软筛选”机制 [9]。Dorfleitner 等直接验证了描述文本相关软信息的增量价值 [10]。

这一路线的关键变化是从“借款人硬信息评分”走向“硬信息 + 软信息联合建模”。本文选用 `desc` 字段，正是对这一研究传统的延续。

### C. 文本软信息建模：从特征工程到深度语义
Jiang 等、Yao 等、Wang 等（ECRA）和 Xia 等将文本软信息系统引入违约预测，表明叙事文本、标题文本与语义特征均可带来增益 [11]-[14]。Wang 等（JMIS）进一步从语义软因子角度构建更强的文本风险特征 [15]。Kriebel 与 Stitz 则用深度学习建模用户文本，进一步提升文本利用效率 [16]。

上述工作共同说明：文本并非噪声字段，其价值取决于表示学习能力、样本规模和业务口径设置。文本模型在离线指标上常有提升，但在固定阈值与稳定运营约束下，其收益可能收敛。

### D. LLM 阶段：能力扩展与落地约束
Transformer、BERT、LoRA 与 GPT-4 代表了文本建模能力的连续升级 [18]-[21]。与此同时，金融场景研究也在扩大数字行为和非传统信息源的使用范围 [22]。近期工作已开始讨论“文本因子驱动的 P2P 风险管理” [17]。

但在风控落地中，LLM 仍面临三类约束：

1. 决策稳定性：阈值迁移后是否仍满足目标拒绝率。  
2. 误差结构：是否在提升召回的同时控制放行坏账率。  
3. 训练信号质量：教师标签与理由文本是否足够多样、真实。  

### E. 文献综合：阶段演进与局限传递
| 阶段 | 代表文献 | 主要贡献 | 典型局限 | 后续阶段如何改进 |
|---|---|---|---|---|
| 经典评分 | [1], [2], [3], [4] | 建立信用评分评估范式与基线 | 对软信息覆盖不足 | 引入平台行为与软信息 |
| P2P 实证 | [6], [7], [8], [9], [10] | 证明信息不对称与软筛选存在 | 文本利用深度不足 | 文本语义建模与融合 |
| 文本特征工程 | [11], [12], [13], [14] | 量化文本增量价值 | 语义表达能力受限 | 深度文本与上下文建模 |
| 深度文本 | [15], [16] | 提升上下文建模与泛化能力 | 解释与阈值控制仍困难 | LLM + 参数高效微调 |
| LLM 应用 | [17], [18], [19], [20], [21] | 强语义理解与解释生成 | 业务稳定性与校准难题 | 强化校准、蒸馏与人机协同 |

### F. 方法比较：为何同为“文本模型”结论仍不一致
已有研究对文本软信息的收益结论并不完全一致，关键原因通常不是“谁的模型更先进”，而是任务定义与实验协议不同。为了避免“跨口径比较”误导，本文将文献中的关键差异归纳为四类：

| 比较维度 | 常见做法差异 | 对结论的影响 |
|---|---|---|
| 标签策略 | 是否只保留 Fully Paid / Charged Off，是否混入进行中贷款 | 混入未终局样本会引入标签噪声，夸大或掩盖文本贡献 |
| 评估口径 | AUC/PR-AUC vs 固定拒绝率业务指标 | 仅看排序指标可能高估上线可用性 |
| 文本粒度 | 标题短文本、描述长文本、二者融合 | 长文本更富信息，但训练难度与噪声控制要求更高 |
| 特征融合 | 早期拼接 vs 联合训练 vs 两阶段决策 | 融合方式决定结构化强信号是否被文本有效补充 |

例如，Dorfleitner 等和 Jiang 等强调“文本有增量价值” [10], [11]，而 Kriebel 与 Stitz 进一步显示深度模型可提升文本利用效率 [16]。但这类结论多在离线分类指标下给出；当评估切换到固定拒绝率、审批阈值稳定性与放行坏账率时，模型间排序可能发生变化。本文正是围绕这种“指标迁移导致的结论迁移”做统一实证。

### G. 本文定位
基于上述脉络，本文不试图证明“LLM 永远优于传统模型”，而是回答更实际的问题：在统一业务口径（固定拒绝率）下，哪类模型更适合作为主判，哪类模型更适合作为解释层。

## IV. Dataset, Task, and Feature Engineering
### A. 数据来源与样本构建
原始数据来自 LendingClub 历史公开数据 [23]，项目中通过 `workflow_common.py` 与各流水线脚本完成统一清洗与共享切分。

| 阶段 | 样本量 | 说明 |
|---|---:|---|
| 原始读取 | 2,260,701 | `accepted_2007_to_2018Q4.csv` |
| 初筛后 | 123,293 | 满足基础状态与字段条件 |
| 清洗后 | 123,202 | 去重/缺失清洗后可用样本 |
| 共享高风险子集 | 7,108 | `grade in {E,F,G}`, `int_rate>=18.5`, `annual_inc<=92,500` |

### B. 标签定义与任务
标签定义与课程项目一致：

- `target=1`: Charged Off（违约）  
- `target=0`: Fully Paid（履约）  

任务是二分类违约识别，但评估目标是“固定拒绝率运营口径下的风险识别质量”。这与传统仅看 AUC 的离线比较不同。

### C. 子集分布与难度特征
共享子集违约率约 `35.00%`，且按规则限定在较高风险区间。该设定的影响是：

1. 结构化风险信号（利率、等级、DTI 等）更集中。  
2. 文本信息仍有价值，但边际增益被强结构化信号压缩。  

| 维度 | 统计 |
|---|---|
| 样本总数 | 7,108 |
| 违约率 | 35.00% |
| 年份范围 | 2008-2016 |
| Grade 分布 | E: 4,414 / F: 2,195 / G: 499 |
| Grade 违约率 | E: 32.85% / F: 37.63% / G: 42.48% |

### D. 共享切分与防泄露控制（口径 A）
统一采用分层随机切分（共享文件位于 `data/shared/splits/stratified`）：

- `train_fit=4548`  
- `validation=1138`  
- `test=1422`  

此外通过 `_shared_row_id` 对齐不同流水线样本，确保各模型使用完全一致的测试集，不发生“模型对比时样本不一致”的隐性偏差。

### E. 全量切分与业务分布口径（口径 B）
为评估模型在更接近真实业务分布下的表现，本文补充全量口径实验：

- 输入：`ml_standalone/data/processed/ml_full_processed.csv`（`n=123,202`）  
- 共享切分目录：`data/shared/splits/full_actual_rr`  
- 切分规模：`train_fit=78848`, `validation=19713`, `test=24641`  
- 目标拒绝率：`RR_target=0.15327673252057597`（全量真实违约率）  

该口径目前覆盖 `ML + DataAnalysis`，用于回答“当样本分布回到全量业务形态时，主模型排序是否变化”。

### F. 特征体系
| 特征组 | 代表字段 | 在本项目中的作用 |
|---|---|---|
| 数值特征 | `loan_amnt`, `int_rate`, `annual_inc`, `dti`, `fico_mean` | 主风险强信号 |
| 类别特征 | `grade`, `home_ownership`, `purpose`, `verification_status` | 结构化分群 |
| 文本特征 | `desc_clean` / `bert_input` | 软信息与语义补充 |
| 流程特征 | `_shared_row_id`, `issue_year` | 对齐切分与误差诊断 |

## V. Methods
### A. 统一方法框架
本文比较四类方法族：

1. `DataAnalysis` 风险画像分类器（规则化分数与阈值策略）。  
2. 传统 ML（逻辑回归、XGBoost）。  
3. BERT 路线（Embedding + 逻辑回归；端到端微调）。  
4. LLM 路线（两阶段微调、单阶段微调、外部 GPT-5.2 推理）。  

所有方法都遵循“验证集定阈值，测试集仅评估”的流程，不用测试集调参。

`DataAnalysis` 不是简单规则打分，而是基于统一特征选择结果生成风险画像分值，并在验证集上按固定拒绝率目标选择阈值；它在本项目中承担“可解释、低复杂度、可落地”的业务参照基线。

### B. 传统 ML 基线
对应代码：`ml_standalone/run_ml_pipeline.py`。

- 预处理：
  - 数值特征：`median imputation + StandardScaler`  
  - 类别特征：`most_frequent imputation + OneHotEncoder`  
  - 文本融合模型：`TF-IDF(1-2gram, min_df=5, max_features=30000)`  
- 候选模型：
  - `logistic_tabular`（`max_iter=1800`, `class_weight=balanced`）  
  - `logistic_text_fusion`（结构化 + TF-IDF）  
  - `xgboost_tabular`（`n_estimators=260`, `max_depth=6`, `learning_rate=0.05` 等）

ML 模型使用 `cv_folds=3` 的分层交叉验证报告平衡准确率，模型筛选仍基于验证集固定拒绝率指标完成。

传统 ML 结果显示一个重要现象：`logistic_text_fusion`（结构化+TF-IDF）并未显著超过 `logistic_tabular`。这意味着在当前高风险子集中，结构化强信号已解释了大部分可分性，文本特征的边际贡献受限于样本规模、文本质量和融合方式。

### C. BERT 路线
对应代码：`bert_standalone/run_bert_pipeline.py`, `bert_standalone/bert_workflow.py`。

1. **Embedding 路线**：`bce-embedding-base_v1` 编码 `bert_input`，再接逻辑回归分类器。  
2. **Finetune 路线**：同底座模型端到端微调（`learning_rate=2e-5`, `train_batch_size=8`, `eval_batch_size=16`, `num_train_epochs=1.0`, `weight_decay=0.01`）。  
3. 输入长度：请求 `max_length=514`，实际按 tokenizer 限制使用 `512`。  

### D. LLM 路线（两阶段/单阶段/外部）
对应代码：`llm_standalone/run_llm_two_stage.py`、`llm_standalone/run_llm_one_stage.py`，一键脚本：`run_llm_two_stage_full_06b.ps1` 与 `run_llm_one_stage_full_06b.ps1`。

流程：

- 两阶段：Stage-1 `Input -> Reason`，Stage-2 `Input + Reason -> Action`。  
- 单阶段：`Input -> Action` 直接给出 reject/approve。  
- 外部 GPT-5.2：基于 `data/loan_decisions.csv` 与共享测试集按 `_shared_row_id` 对齐评估。  

训练配置（LlamaFactory 配置文件）：

- 学生模型：`Qwen3-0.6B`  
- LoRA：`rank=8`  
- `cutoff_len=2048`, `per_device_train_batch_size=1`, `gradient_accumulation_steps=8`, `learning_rate=1e-4`, `fp16=true`  
- 阶段 epoch：Reason=2.0，Action=4.0

推理阶段使用 reject/approve 的 logprob 计算拒绝概率。两阶段支持验证集定阈重标定；单阶段与外部 GPT-5.2 当前输出在共享口径下存在拒绝率偏移，需要单独标注公平性风险。

### E. 指标定义与业务意义
以“拒绝”为正类，本文在两套目标拒绝率下评估：共享子集 `RR_target=0.35`，全量口径 `RR_target=0.15327673252057597`。

- `Precision@RR = TP / (TP + FP)`：被拒样本中真实违约占比。  
- `Recall@RR = TP / (TP + FN)`：违约捕获率。  
- `Approval Bad Rate = FN / (TN + FN)`：放行样本中的坏账率。  
- `Lift@RR = Precision@RR / RR_target`：相对随机拒绝的提升倍数（口径内比较）。  

这些指标能够直接映射“误拒”与“误放”的业务权衡，比单一 AUC 更贴近审批流程；但不同 `RR_target` 下的绝对值不应直接横向比较。

### F. 统一阈值选择流程（防止口径漂移）
为确保不同模型可公平比较，本文采用同一阈值流程：

1. 在验证集上按目标拒绝率寻找分位点阈值（或等价概率阈值）。  
2. 记录该阈值下的 `Precision@RR`、`Recall@RR`、`Approval Bad Rate`。  
3. 固定阈值后一次性在测试集评估，不再回看测试结果调阈。  
4. 对 LLM 额外报告“默认阈值 vs 固定 RR 阈值”的敏感性，以分离“模型能力”与“阈值校准”影响。  

## VI. Experiment Design and Statistical Protocol
### A. 公平比较控制
- 口径 A（公平横评）：`data/shared/shared_subset.csv` + `data/shared/splits/stratified` + `RR=35%`  
- 口径 B（业务分布）：`ml_full_processed.csv` + `data/shared/splits/full_actual_rr` + `RR=0.1532767`  
- 同一阈值流程：验证集定阈 -> 测试集固定评估  
- 对拒绝率显著偏离目标的模型单独标注（不并入公平主榜）  

### B. 实验阶段
1. **主对比实验（口径 A）**：7 条对齐路线（DA/3个ML/BERT-Emb/BERT-FT/LLM-2stage）。  
2. **扩展实验（口径 A）**：LLM 单阶段与 GPT-5.2（报告但不纳入公平主榜）。  
3. **拒绝率扫描实验**：在 `10%/20%/35%/50%` 口径比较模型排序变化。  
4. **LLM 阈值敏感性**：对比默认阈值与固定 RR 重标定。  
5. **不确定性评估**：对关键模型做 bootstrap 95% CI（`B=1000`）。  
6. **全量口径实验（口径 B）**：ML + DataAnalysis 在实际违约率目标下比较。

### C. 实证规范说明（与课程最佳实践对齐）
- 无任何测试集调参。  
- 交叉验证仅用于训练阶段参考（ML），最终阈值由验证集业务目标确定。  
- 不同模型输出通过相同业务指标解释，避免“指标口径漂移”。  

### D. 统计与稳健性设计
为避免“单次切分偶然性”导致结论不稳，本文补充两类稳健性检查：

1. **拒绝率扫描**：在 `10%/20%/35%/50%` 下比较精度排序是否一致。  
2. **Bootstrap 置信区间**：在测试集上对关键模型进行 `B=1000` 次重采样，报告 95% CI。  

其中，拒绝率扫描检验的是“运营策略变化下模型排序的可迁移性”；Bootstrap 检验的是“在当前测试规模（`n=1422`）下结论是否具有统计稳定性”。

## VII. Experimental Results
### A. 主结果（口径 A：共享子集，RR 目标=35%，公平主榜）
| 模型 | Precision@RR | Recall@RR | Approval Bad Rate | Lift@RR | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| DataAnalysis | 0.4175 | 0.4317 | 0.3120 | 1.1928 | 0.3622 |
| ML: logistic_tabular | 0.4154 | 0.4538 | 0.3098 | 1.1870 | 0.3826 |
| ML: logistic_text_fusion | 0.4154 | 0.4337 | 0.3126 | 1.1868 | 0.3657 |
| ML: xgboost_tabular | 0.4030 | 0.4378 | 0.3178 | 1.1513 | 0.3805 |
| BERT-Embedding | 0.3859 | 0.4177 | 0.3284 | 1.1026 | 0.3790 |
| BERT-Finetune | 0.3895 | 0.4317 | 0.3253 | 1.1128 | 0.3882 |
| LLM Two-Stage (Qwen3-0.6B) | 0.3653 | 0.3675 | 0.3420 | 1.0436 | 0.3523 |

在拒绝率接近 35% 的公平约束下，`DataAnalysis` 与 `logistic_tabular` 仍是最稳健路线；LLM 两阶段表现可用但与结构化主线存在明显差距。

### B. 扩展 LLM 对比（口径 A：同测试集，但未全部满足 RR 对齐）
| 模型 | Precision@RR | Recall@RR | Approval Bad Rate | Lift@RR | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| LLM Two-Stage (Qwen3-0.6B) | 0.3653 | 0.3675 | 0.3420 | 1.0436 | 0.3523 |
| LLM One-Stage (Qwen3-0.6B) | 0.3587 | 0.6245 | 0.3369 | 1.0249 | 0.6097 |
| GPT-5.2 (External, no finetune) | 0.3462 | 0.8133 | 0.3690 | 0.9884 | 0.8228 |

扩展结果说明：单阶段 LLM 与 GPT-5.2 的高召回主要来自“高拒绝率策略”，并未满足 `RR=35%` 的公平运营约束，因此不并入主榜结论。

### C. 全量业务口径结果（口径 B：n=123,202，RR 目标=0.1533）
| 模型 | Precision@RR | Recall@RR | Approval Bad Rate | Lift@RR | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML: logistic_tabular | 0.3069 | 0.2992 | 0.1263 | 2.0023 | 0.1494 |
| ML: logistic_text_fusion | 0.2994 | 0.3050 | 0.1262 | 1.9532 | 0.1562 |
| ML: xgboost_tabular | 0.3153 | 0.3095 | 0.1246 | 2.0568 | 0.1505 |
| DataAnalysis | 0.2849 | 0.2851 | 0.1294 | 1.8589 | 0.1534 |

在全量分布下，`xgboost_tabular` 取代 `logistic_tabular` 成为最优，这表明主模型排序与样本分布、`RR_target` 设定有关。

### D. 拒绝率扫描：模型排序是否稳定（口径 A）
| 模型 | RR=10% Precision | RR=20% Precision | RR=35% Precision | RR=50% Precision |
|---|---:|---:|---:|---:|
| ML: logistic_tabular | 0.5180 | 0.4712 | 0.4154 | 0.4023 |
| DataAnalysis | 0.4797 | 0.4254 | 0.4175 | 0.3992 |
| ML: xgboost_tabular | 0.4248 | 0.4415 | 0.4030 | 0.3963 |
| BERT-Finetune | 0.4044 | 0.3789 | 0.3895 | 0.3826 |

观察：低拒绝率区间（10%-20%）下 `logistic_tabular` 优势最明显；随着拒绝率提高，模型精度差距缩小，但结构化路线仍保持相对领先。

### E. 两阶段 LLM 阈值敏感性
| 设置 | Reject Rate | Precision | Recall | Approval Bad Rate | Lift |
|---|---:|---:|---:|---:|---:|
| 默认阈值（delta>=0） | 0.2820 | 0.3691 | 0.2972 | 0.3428 | 1.0545 |
| 固定 RR=35%（验证集重标定） | 0.3523 | 0.3653 | 0.3675 | 0.3420 | 1.0436 |

结果显示，重标定主要改善了召回（+7.03 个百分点），但 Precision 略降，且放行坏账率改善有限。

### F. 不确定性（Bootstrap 95% CI）
| 模型 | Precision (95% CI) | Recall (95% CI) | Approval Bad Rate (95% CI) |
|---|---|---|---|
| DataAnalysis | 0.4175 ([0.3735, 0.4605]) | 0.4317 ([0.3906, 0.4767]) | 0.3120 ([0.2834, 0.3428]) |
| ML: logistic_tabular | 0.4154 ([0.3766, 0.4532]) | 0.4538 ([0.4094, 0.5000]) | 0.3098 ([0.2793, 0.3395]) |
| BERT-Finetune | 0.3895 ([0.3498, 0.4328]) | 0.4317 ([0.3861, 0.4807]) | 0.3253 ([0.2957, 0.3573]) |
| LLM Two-Stage FixedRR | 0.3653 ([0.3245, 0.4077]) | 0.3675 ([0.3273, 0.4088]) | 0.3420 ([0.3131, 0.3706]) |

置信区间结果与主结论一致：LLM 与结构化基线之间仍存在明显性能差距，尤其在 Precision 与 Approval Bad Rate 上。

### G. 误差结构视角：每千笔申请的业务代价（口径 A）
为了将指标转换为业务可读语义，我们将主结果换算为“每 1000 笔申请”的近似误差结构：

| 模型 | 每1000笔捕获违约（TP） | 每1000笔放过违约（FN） | 每1000笔误拒好人（FP） |
|---|---:|---:|---:|
| ML: logistic_tabular | 158.9 | 191.3 | 223.6 |
| DataAnalysis | 151.2 | 199.0 | 211.0 |
| BERT-Finetune | 151.2 | 199.0 | 237.0 |
| LLM Two-Stage (Qwen3-0.6B) | 128.7 | 221.5 | 223.6 |

关键结论是：`logistic_tabular` 与两阶段 LLM 在误拒好人规模上几乎相当（`223.6` vs `223.6`），但前者每 1000 笔可额外捕获约 `30.2` 笔违约。这说明当前差距主要来自“违约识别能力不足”，而非“过度拒绝”。

## VIII. Discussion and Analysis
### A. 为什么当前结构化模型仍占优
1. **样本选择效应**：共享子集通过高风险规则筛选，结构化信号浓度提升。  
2. **训练信号约束**：两阶段 LLM 的教师理由来自规则化模板，语义多样性不足。  
3. **决策头限制**：Stage-2 最终为二元动作比较，难充分利用复杂上下文。  
4. **模型规模与样本规模错配**：`0.6B` 在当前数据量下更易表现为“可解释但判别力有限”。  
5. **校准约束差异**：单阶段 LLM 与 GPT-5.2 在当前配置下表现为“高拒绝率换高召回”，难直接满足固定 RR 运营规则。  

### B. 与文献脉络的对应关系
文献已显示文本软信息可提供增量价值 [10]-[17]，但这些增益高度依赖任务定义与评估口径。本文结果并不否定文本价值，而是说明在“固定拒绝率运营”约束下，文本模型增益需要更强训练策略与更细致校准才可能稳定体现；在“全量真实分布”口径下，结构化模型内部排序也会发生变化（`xgboost_tabular` 领先）。

### C. 哪些方向更有前景
1. **更高质量教师信号**：从规则模板升级到专家审阅理由，减少模式坍塌。  
2. **联合建模**：将结构化强信号与文本语义在同一判别头融合，而非简单串联。  
3. **校准优先**：引入温度缩放/分段校准与 top-k 约束，优先优化固定 RR 下的业务指标。  
4. **时序外推验证**：补充时间外推切分，验证跨年份稳定性。  

### D. 哪些方向前景有限
1. 仅靠更复杂提示词而不改变训练信号质量，边际收益通常有限。  
2. 在无校准前提下直接将 LLM 作为主判，会放大运营波动与审核负担。  
3. 将“高拒绝率高召回”结果直接宣称为更优策略，会忽略业务约束与客户体验成本。  

### E. 威胁与有效性（Threats to Validity）
1. **样本代表性限制**：主实验聚焦高风险子集，结论不直接代表全量客群；但该设置有助于回答“高风险审批”场景问题。  
2. **时序外推未完全验证**：当前以分层随机切分为主，尚未进行严格按时间滚动测试；因此跨周期稳定性仍需补充实验。  
3. **LLM 教师信号偏差**：两阶段数据由教师流程构造，若理由文本模式较单一，可能限制学生模型泛化。  
4. **文本字段历史缺失**：`desc` 在历史期间存在可用性差异，这会影响时间段之间的可比性。  

### F. 与评分标准的对应说明
按课程评分维度看，本文通过以下方式对齐要求：

1. **全面性**：从经典评分、P2P 信息不对称、文本软信息、深度文本到 LLM 连续梳理，并给出阶段演进表。  
2. **分析深度**：不只列文献，而是分析“任务定义/评估协议”如何改变结论。  
3. **实证质量**：统一切分、统一阈值、无测试调参，并报告拒绝率扫描与置信区间。  
4. **清晰度与代码一致性**：将主要结论映射到具体脚本与输出文件，支持审计与复现。  

## IX. Code Organization and Reproducibility
### A. 代码结构与职责映射
| 目录/脚本 | 职责 |
|---|---|
| `workflow_common.py` | 清洗、切分、阈值优化、统一指标函数 |
| `ml_standalone/run_ml_pipeline.py` | 传统 ML 训练、验证定阈、测试评估 |
| `data_analysis_standalone/run_data_analysis_classifier.py` | 数据分析规则化风险评分基线 |
| `bert_standalone/run_bert_pipeline.py` | BERT embedding/finetune 训练与评估 |
| `llm_standalone/run_llm_two_stage.py` | 两阶段 LLM 数据构建、训练配置、推理评估 |
| `llm_standalone/run_llm_one_stage.py` | 单阶段 LLM 训练与推理评估 |
| `llm_standalone/run_llm_two_stage_full_06b.ps1` | 两阶段 LLM 一键流程脚本 |
| `llm_standalone/run_llm_one_stage_full_06b.ps1` | 单阶段 LLM 一键流程脚本 |
| `6800paper/scripts/md_to_ieee_docx.py` | 从 `6800main.md` 自动生成 IEEE 双栏 Word |

### B. 一致性与复现实证
本文中所有关键结论均可从以下文件复现：

- 主结果：`output/doc/report_tables/main_results.csv`  
- 拒绝率扫描：`output/doc/report_tables/reject_rate_sweep.csv`  
- LLM 阈值敏感性：`output/doc/report_tables/llm_threshold_sensitivity.csv`  
- 置信区间：`output/doc/report_tables/bootstrap_ci_selected_models.csv`  
- 单阶段 LLM：`output/llm_one_stage_subset_full_06b/infer/stage_action_direct_predictions_full.json`  
- 外部 GPT-5.2：`output/llm_two_stage_subset_full_06b/infer/gpt52_external_metrics.json`  
- 全量口径 ML：`output/ml_standalone_full_actual_rr/run_report.json`  
- 全量口径 DataAnalysis：`output/data_analysis_standalone_full_actual_rr/run_report.json`  

### C. 复现步骤（建议写入提交说明）
1. 重建共享切分并运行 `ML + DataAnalysis + BERT`。  
2. 运行 `run_llm_two_stage_full_06b.ps1` 与 `run_llm_one_stage_full_06b.ps1`。  
3. 运行外部预测评估脚本并更新 `gpt52_external_metrics.json`。  
4. 运行全量口径 `ML + DataAnalysis`。  
5. 更新 `6800main.md` 后执行 `md_to_ieee_docx.py` 生成最终文档。  

## X. Conclusion
本文在统一数据、统一切分、统一阈值流程下，基于“共享子集公平口径 + 全量业务口径”对传统模型、BERT、两阶段/单阶段 LLM 与外部 GPT-5.2 进行了系统比较。实验结果表明：

1. 结构化模型（尤其 `logistic_tabular` 与 `DataAnalysis`）在固定拒绝率场景下仍更稳健。  
2. 两阶段 LLM 的阈值重标定可以显著提升召回，但不足以弥补整体判别差距；单阶段 LLM 与 GPT-5.2 当前主要通过高拒绝率获取高召回。  
3. 全量真实分布口径下 `xgboost_tabular` 综合最优，说明模型排序受样本分布与目标 RR 影响。  
4. 当前最可行的工程路线是“结构化主判 + LLM 解释辅助 + 人工复核联动”。  

从课程项目视角看，本文的主要价值不在于“追求单一最高分”，而在于建立了与业务约束一致、与代码产物一致、可复现可审计的比较框架。这一框架同样可用于后续更大模型或更复杂融合方法的公平评估。

## References
[1] D. J. Hand and W. E. Henley, “Statistical Classification Methods in Consumer Credit Scoring: A Review,” *Journal of the Royal Statistical Society Series A: Statistics in Society*, vol. 160, no. 3, pp. 523-541, 1997, doi: 10.1111/j.1467-985X.1997.00078.x.  
[2] L. C. Thomas, “A survey of credit and behavioural scoring: forecasting financial risk of lending to consumers,” *International Journal of Forecasting*, vol. 16, no. 2, pp. 149-172, 2000, doi: 10.1016/S0169-2070(00)00034-0.  
[3] A. E. Khandani, A. J. Kim, and A. W. Lo, “Consumer credit-risk models via machine-learning algorithms,” *Journal of Banking & Finance*, vol. 34, no. 11, pp. 2767-2787, 2010, doi: 10.1016/j.jbankfin.2010.06.001.  
[4] S. Lessmann, B. Baesens, H.-V. Seow, and L. C. Thomas, “Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research,” *European Journal of Operational Research*, vol. 247, no. 1, pp. 124-136, 2015, doi: 10.1016/j.ejor.2015.05.030.  
[5] T. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2016, pp. 785-794, doi: 10.1145/2939672.2939785.  
[6] M. Lin, N. R. Prabhala, and S. Viswanathan, “Judging Borrowers by the Company They Keep: Friendship Networks and Information Asymmetry in Online Peer-to-Peer Lending,” *Management Science*, vol. 59, no. 1, pp. 17-35, 2013, doi: 10.1287/mnsc.1120.1560.  
[7] C. Serrano-Cinca, B. Gutiérrez-Nieto, and L. López-Palacios, “Determinants of Default in P2P Lending,” *PLOS ONE*, vol. 10, no. 10, p. e0139427, 2015, doi: 10.1371/journal.pone.0139427.  
[8] R. Emekter, Y. Tu, B. Jirasakuldech, and M. Lu, “Evaluating credit risk and loan performance in online Peer-to-Peer (P2P) lending,” *Applied Economics*, vol. 47, no. 1, pp. 54-70, 2015, doi: 10.1080/00036846.2014.962222.  
[9] R. Iyer, A. I. Khwaja, E. F. P. Luttmer, and K. Shue, “Screening Peers Softly: Inferring the Quality of Small Borrowers,” *Management Science*, vol. 62, no. 6, pp. 1554-1577, 2016, doi: 10.1287/mnsc.2015.2181.  
[10] G. Dorfleitner, C. Priberny, S. Schuster, J. Stoiber, M. Weber, I. de Castro, and J. Kammler, “Description-text related soft information in peer-to-peer lending - Evidence from two leading European platforms,” *Journal of Banking & Finance*, vol. 64, pp. 169-187, 2016, doi: 10.1016/j.jbankfin.2015.11.009.  
[11] C. Jiang, Z. Wang, R. Wang, and Y. Ding, “Loan default prediction by combining soft information extracted from descriptive text in online peer-to-peer lending,” *Annals of Operations Research*, vol. 266, no. 1-2, pp. 511-529, 2018, doi: 10.1007/s10479-017-2668-z.  
[12] J. Yao, J. Chen, J. Wei, Y. Chen, and S. Yang, “The relationship between soft information in loan titles and online peer-to-peer lending: evidence from RenRenDai platform,” *Electronic Commerce Research*, vol. 19, no. 1, pp. 111-129, 2019, doi: 10.1007/s10660-018-9293-z.  
[13] C. Wang, W. Zhang, X. Zhao, and J. Wang, “Soft information in online peer-to-peer lending: Evidence from a leading platform in China,” *Electronic Commerce Research and Applications*, vol. 36, p. 100873, 2019, doi: 10.1016/j.elerap.2019.100873.  
[14] Y. Xia, L. He, Y. Li, N. Liu, and Y. Ding, “Predicting loan default in peer-to-peer lending using narrative data,” *Journal of Forecasting*, vol. 39, no. 2, pp. 260-280, 2020, doi: 10.1002/for.2625.  
[15] Z. Wang, C. Jiang, H. Zhao, and Y. Ding, “Mining Semantic Soft Factors for Credit Risk Evaluation in Peer-to-Peer Lending,” *Journal of Management Information Systems*, vol. 37, no. 1, pp. 282-308, 2020, doi: 10.1080/07421222.2019.1705513.  
[16] J. Kriebel and L. Stitz, “Credit default prediction from user-generated text in peer-to-peer lending using deep learning,” *European Journal of Operational Research*, vol. 302, no. 1, pp. 309-323, 2022, doi: 10.1016/j.ejor.2021.12.024.  
[17] M. Siering, “Peer-to-Peer (P2P) Lending Risk Management: Assessing Credit Risk on Social Lending Platforms Using Textual Factors,” *ACM Transactions on Management Information Systems*, vol. 14, no. 3, pp. 1-19, 2023, doi: 10.1145/3589003.  
[18] A. Vaswani *et al*., “Attention Is All You Need,” in *Advances in Neural Information Processing Systems 30*, 2017, doi: 10.48550/arXiv.1706.03762.  
[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” in *Proceedings of NAACL-HLT*, 2019, pp. 4171-4186, doi: 10.18653/v1/N19-1423.  
[20] E. J. Hu *et al*., “LoRA: Low-Rank Adaptation of Large Language Models,” arXiv:2106.09685, 2021, doi: 10.48550/arXiv.2106.09685.  
[21] OpenAI *et al*., “GPT-4 Technical Report,” arXiv:2303.08774, 2023, doi: 10.48550/arXiv.2303.08774.  
[22] T. Berg, V. Burg, A. Gombović, and M. Puri, “On the Rise of FinTechs: Credit Scoring Using Digital Footprints,” *The Review of Financial Studies*, vol. 33, no. 7, pp. 2845-2897, 2020, doi: 10.1093/rfs/hhz099.  
[23] LendingClub, “LendingClub Statistics and Loan Data,” [Online]. Available: https://www.lendingclub.com/info/download-data.action (archived: https://web.archive.org/web/20110816083005/https://www.lendingclub.com/info/download-data.action). [Accessed: Feb. 21, 2026].

## Appendix A: Reproducible Artifacts
- `data/shared/shared_subset.csv`
- `data/shared/shared_subset.summary.json`
- `data/shared/splits/stratified/split_summary.json`
- `output/ml_standalone/run_report.json`
- `output/data_analysis_standalone/run_report.json`
- `output/bert_standalone/run_report.json`
- `output/llm_two_stage_subset_full_06b/infer/stage2_action_predictions_full.json`
- `output/llm_one_stage_subset_full_06b/infer/stage_action_direct_predictions_full.json`
- `output/llm_two_stage_subset_full_06b/infer/gpt52_external_metrics.json`
- `output/ml_standalone_full_actual_rr/run_report.json`
- `output/data_analysis_standalone_full_actual_rr/run_report.json`
- `output/doc/report_tables/main_results.csv`
- `output/doc/report_tables/reject_rate_sweep.csv`
- `output/doc/report_tables/llm_threshold_sensitivity.csv`
- `output/doc/report_tables/bootstrap_ci_selected_models.csv`
