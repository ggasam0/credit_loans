# 6800 PPT 演讲稿（18 分钟详讲版 v2）

> 对应文件：`6800paper/6800_final_presentation_v5.pptx`  
> 目标总时长：约 18 分钟（1080 秒）  
> 建议语速：关键数字处减速，结论句停顿 1 秒

## 时间分配总览
- Slide 1：50 秒
- Slide 2：60 秒
- Slide 3：110 秒
- Slide 4：100 秒
- Slide 5：105 秒
- Slide 6：120 秒
- Slide 7：120 秒
- Slide 8：120 秒
- Slide 9：95 秒
- Slide 10：95 秒
- Slide 11：55 秒
- Slide 12：50 秒

---

## Slide 1 封面（约 50 秒）
大家好，我是 `<姓名/学号>`。今天汇报的主题是“基于借款描述文本的信用违约评估：传统模型与两阶段 LLM 的统一口径实验”。

我想先明确这项工作的目标：我们不是做一个“谁分数更高”的排行榜，而是回答一个上线前必须回答的问题。在统一数据、统一切分、统一阈值流程下，哪类模型可以作为稳定主判，哪类模型应该定位为解释和复核辅助。

这个目标看起来保守，但对风控场景更真实。因为真正上线时，拒绝率、坏账代价、可解释性和合规边界都要同时满足。

过渡：先看整场汇报怎么组织。

## Slide 2 报告目录（约 60 秒）
汇报分为九个模块：研究动机、文献定位、数据与任务、方法协议、口径 A 主结果、扩展 LLM 与口径 B、稳健性分析、讨论与局限、可复现性与总结。

请大家带着一条主线听：固定拒绝率约束下的公平比较。我们会多次回到这个约束，因为它决定了“高召回是否值得”。

此外，本报告用了双口径：口径 A 强调公平横评，口径 B 强调真实部署分布。两者是互补关系，不是二选一关系。

过渡：先从业务问题本身讲起。

## Slide 3 研究动机与核心问题（约 110 秒）
信贷风控的难点是信息不对称。结构化变量如收入、负债、信用历史很重要，但不完整。借款描述文本通常会包含“用途、还款计划、临时压力”等软信息，这部分信息如果处理得好，可能提升风险识别。

但文本建模有两个现实问题：
- 第一，文本质量不稳定，噪声很高；
- 第二，模型越复杂，越容易出现“离线分数高、上线不稳定”的现象。

所以我们提出三个研究问题。

RQ1：在固定拒绝率下，文本模型是否稳定优于结构化模型？
RQ2：当拒绝率目标变化时，模型排序是否迁移？
RQ3：两阶段 LLM 经过重标定后，是否带来真实业务收益？

这三问对应三个贡献。
- 贡献一：统一代码和评估口径，避免“不同实验脚本之间不可比”。
- 贡献二：共享子集与全量分布双口径并列报告，把模型能力和分布影响分开。
- 贡献三：补充 RR 扫描、阈值敏感性和 Bootstrap 区间，不只看点估计。

一句话概括：我们把“模型实验”提升为“可部署决策实验”。

过渡：再看本文在文献中的具体站位。

## Slide 4 文献脉络与本文定位（约 100 秒）
相关研究大体经历三段。

第一段是传统信用评分阶段，以结构化特征为核心，优势是稳健、可解释、流程成熟。

第二段是文本增强阶段，利用主题模型或深度编码把借款描述转成特征，很多工作报告了离线增益。

第三段是 LLM 阶段，开始探索 prompt、理由生成、两阶段决策等新方法，优势是语义理解更强，但也带来成本、稳定性和校准问题。

本文的定位不是“证明 LLM 必然领先”。我们的立场是：只在同数据、同切分、同拒绝率约束下讨论优劣，并且要求结论可以映射到审批策略。

所以，你可以把这篇工作理解成“方法比较 + 决策口径治理”的结合，而不是纯建模竞赛。

过渡：下面进入数据和任务定义，这决定了后续所有结论的边界。

## Slide 5 数据集、标签与双口径设置（约 105 秒）
标签定义采用行业常见二分类：`target=1` 是 `Charged Off`，`target=0` 是 `Fully Paid`。任务是固定拒绝率下识别违约风险，兼顾放行质量。

我们设置两个口径。

口径 A：共享子集 + RR=35%。目标是公平比较，不让模型靠不同拒绝规模“换分数”。

口径 B：全量样本 n=123,202 + RR=0.1533。目标是接近真实部署分布，观察模型在实际基准坏账率下的表现。

为什么必须双口径？因为单口径会混淆两类因素：
- 模型能力差异；
- 分布和策略目标差异。

如果不区分这两类因素，常见错误是把“离线横评冠军”直接当成“线上主判冠军”，结果上线后发现指标和代价都偏离预期。

过渡：接下来是方法协议，这是公平比较能否成立的关键。

## Slide 6 方法框架与评估协议（约 120 秒）
模型家族覆盖四类：
- DataAnalysis 规则化基线；
- 传统 ML（logistic_tabular、logistic_text_fusion、xgboost_tabular）；
- BERT 路线；
- LLM 路线（Two-stage、One-stage、External GPT-5.2）。

这里最重要的不是模型名字，而是统一协议：
1) 在训练集训练；
2) 在验证集按目标 RR 选阈值；
3) 固定阈值后在测试集一次性评估；
4) 禁止测试集调参。

这四步保证了可比性和可审计性。

另外我们在方法定位上有意保守：把结构化模型作为主判候选，把 BERT/LLM 重点评估为增益与可部署性。这个定位不是偏见，而是基于当前数据规模、标签质量和稳定性要求做出的工程选择。

如果未来校准和教师信号显著改善，LLM 角色可以提升，但前提是先满足稳定边界。

过渡：先看口径 A 主榜结果。

## Slide 7 主结果（口径 A：共享子集，RR=35%）（约 120 秒）
口径 A 的主榜规则很严格：仅比较 Reject Rate 接近 35% 的模型。目的是在相近代价下比较真正识别能力。

结果里有两个关键点。

第一，`DataAnalysis` Precision 最高（0.4175），说明其拒绝样本命中质量最好。

第二，`logistic_tabular` Recall 更高（0.4538），且 ABR 也更有竞争力，说明其在“抓违约”与“代价控制”之间更平衡。

因此，口径 A 的稳健候选是 DataAnalysis 与 logistic_tabular。两阶段 LLM 虽然可用，但在 Precision 和通过坏账率控制上仍落后。

这里要强调一个答辩常见误区：有人会只看 Recall 说某模型更好。但在审批问题里，如果 Recall 提升是靠更高拒绝率换来的，就不能等价为“业务更好”。这就是我们坚持固定 RR 约束的原因。

过渡：那放到全量真实分布，结论是否变化？

## Slide 8 扩展结果：LLM 对比与全量口径（约 120 秒）
这页分两段讲。

第一段看扩展 LLM。One-stage 和 GPT-5.2 在某些设置下能拉高召回，但 RR 偏移明显，无法满足主榜公平入榜条件。结论不是“LLM 无效”，而是“当前形式下不适合作主判”。

第二段看口径 B 全量分布。n=123,202、RR=0.1533 下，`xgboost_tabular` 成为综合最优。这和口径 A 的排序不完全一致。

这正是本研究最重要的发现之一：模型排序会随着分布与目标 RR 改变。也就是说，“公平横评冠军”和“部署冠军”可能不同，汇报时必须并列呈现。

这一页建议你讲慢一点，因为它直接解释了为什么我们后面要做双榜治理，而不是给一个单一冠军。

过渡：再看稳健性分析，验证这个结论是否可靠。

## Slide 9 稳健性：拒绝率扫描与误差结构（约 95 秒）
稳健性部分回答两件事。

第一，阈值变化时排序是否稳定。我们通过 RR 扫描看模型在不同拒绝率目标下的表现变化，避免只看单点结论。

第二，误差结构是否有业务解释。这里的关键发现是：`logistic_tabular` 与 `LLM Two-Stage` 的 FP 规模接近，但前者每千笔多捕获约 30.2 笔违约。

这意味着当前差距主要来自 TP/FN 结构，而不是“谁拒得更狠”。

这个结论对优化方向非常关键：应该优先提升违约捕获与概率校准，而不是简单提高拒绝率阈值。

过渡：下面进入讨论、局限和下一步优先级。

## Slide 10 讨论、局限与改进优先级（约 95 秒）
为什么结构化主线仍占优，我们给出五点解释：
- 高风险子集强化了结构化强信号；
- 两阶段 LLM 教师理由有模板化现象；
- 二元决策头限制了长文本信息利用；
- 0.6B 模型与当前样本/标注质量存在错配；
- 缺乏校准时容易出现“高拒绝率换高召回”的假优势。

有效性威胁同样要坦诚说明：主实验聚焦高风险子集，外推需谨慎；时间外推验证尚不完整；`desc` 字段存在历史可用性不均衡。

改进优先级建议：
- 高优先：教师信号升级 + 概率校准；
- 中优先：结构化与文本联合判别头；
- 低优先：单纯加复杂提示词。

你在答辩时可以强调：我们不是只“指出问题”，而是给出了按优先级可执行的路线。

过渡：下一页做收束总结。

## Slide 11 关键结论单页（约 55 秒）
这页只讲三句话。

第一，口径 A 最优是 DataAnalysis / logistic_tabular，适合作为固定 RR 约束下的上线候选。

第二，口径 B 最优是 xgboost_tabular，说明真实分布下排序会变化。

第三，LLM 当前更适合解释与复核辅助层，不建议直接主判。

再补一句原则：高召回不等于更优，必须在 RR 约束和误差结构里解释。

过渡：最后给出复现路径和最终结论。

## Slide 12 可复现性与最终结论（约 50 秒）
复现上，我们建立了代码到结论的映射：`workflow_common.py`、`run_ml_pipeline.py`、`run_bert_pipeline.py`、`run_llm_two_stage.py` 各司其职，结果可回查。

推荐复现顺序是四步：先跑传统模型与基线，再跑 LLM 方案，再对齐外部 GPT 结果，最后回写到 `6800main.md` 与最终文档。

最终结论：固定拒绝率公平比较下，结构化模型仍是主判首选；两阶段 LLM 当前更适合作为解释与复核辅助层；下一步是“结构化主判 + LLM 解释 + 人工复核联动”。

谢谢大家，欢迎提问。

---

## 现场缓冲语句（可选）
- 如果讲快了，可在 Slide 7 和 Slide 8 各补 15 秒，强调“主榜与部署榜并列汇报”。
- 如果老师追问“为何不直接上 LLM 主判”，可回答“当前最大问题不是不可用，而是 RR 偏移与校准稳定性未达上线门槛”。
- 如果老师追问“下一步最值钱实验”，可回答“时间外滚动验证 + 概率校准，这是从离线到上线的关键桥梁”。
