# 7000 PPT 演讲稿（初版）

> 对应文件：`7000paper/7000_presentation_v4.pptx`  
> 建议总时长：10-12 分钟（可压缩到 8-9 分钟）

## Slide 1 封面（约 30 秒）
大家好，我是 `<姓名/学号>`。今天汇报的题目是“基于双口径评估的信贷风控顶点项目：结构化模型与文本模型的可落地比较”。这次项目的核心目标不是单纯追求某个指标最高，而是在公平比较和真实部署两个视角下，给出可执行、可解释、可复现的模型决策建议。

过渡：下面先说明我们到底要回答哪些研究问题。

## Slide 2 研究问题与目录（约 60 秒）
本项目围绕四个研究问题展开。第一，在统一流程下，不同模型家族谁更稳健。第二，当我们加入固定拒绝率约束时，模型排序是否会变化。第三，LLM 当前阶段更适合直接主判，还是做解释与复核辅助。第四，在共享高风险子集上得到的结论，能否外推到全量分布。

目录上，我们会先讲双口径评估协议，再看口径 A 的公平主榜、口径 B 的部署结果，然后解释误差结构与业务代价，最后给部署建议和后续路线。

过渡：先看为什么我们要做“双口径”。

## Slide 3 双口径方法设计（约 70 秒）
我们把评估分成两套口径，但执行流程完全一致：先 `train_fit` 训练，再在 `validation` 按目标拒绝率定阈值，最后在 `test` 固定阈值评估，并输出 `run_report.json` 作为可追溯证据。

关键约束是：禁止测试集调参。这样做的目的，是确保不同模型比较时没有“看答案调参”的偏差，也让后续结果在学术和工程上都站得住。

口径 A 用于公平横向比较，口径 B 用于部署真实性能判断。二者回答的是不同问题，所以后面我们会并列汇报，而不是混成一个结论。

过渡：在这套协议下，我们先看参与比较的模型家族。

## Slide 4 模型家族与试点产物（约 70 秒）
我们覆盖四类方法：
第一类是 DataAnalysis 规则化基线，优势是可解释、贴近业务语义。第二类是结构化 ML，包括 logistic 和 xgboost。第三类是 BERT 文本模型。第四类是 LLM 路线，包括 two-stage、one-stage 和 GPT-5.2 变体。

试点通过标准有四条：数据一致、流程一致、结果可回查、偏差可解释。只有同时满足这四条，模型结果才进入后续对比与部署讨论。

过渡：先看口径 A，也就是公平主榜。

## Slide 5 口径 A 主结果（公平主榜）（约 90 秒）
口径 A 的规则是：只比较 Reject Rate 接近 35% 的模型，确保比较“同等拒绝代价”下的质量差异。

在这个约束下，`logistic_tabular` 的 Recall 最高，达到 0.4538，说明它对违约样本捕获能力更强；`DataAnalysis` 的 Precision 略高，为 0.4175，说明它拒绝的人群里命中违约的比例更高。

扩展组里，one-stage LLM 和 GPT-5.2 看起来召回高，但 RR 偏移也显著增大，所以它们不进入主榜结论。这里想强调一个方法论：高召回如果由高拒绝率“买出来”，在业务上往往不可接受。

过渡：那在真实全量分布下，结论会不会变？

## Slide 6 口径 B 结果与误差结构（约 90 秒）
在口径 B，也就是全量真实分布下，最优模型变成了 `xgboost_tabular`，Precision 0.3153，Lift 2.0568。这里 Lift 可以理解为相对基准坏账率的提升倍数。

这页最重要的不是某个数值，而是“排序变化”本身：口径 A 与口径 B 的最优模型不同，说明样本分布和目标函数会直接改变最优解。

所以我们必须把“公平横评结论”和“部署结论”并列汇报，不能用一个场景的冠军直接替代另一个场景。

过渡：既然双口径会给出不同信号，我们就需要明确监控和治理边界。

## Slide 7 监控规则与双榜边界（约 90 秒）
我们使用 `RR_gap = RR_actual - RR_target` 做运行期监控，并按绝对值分层：绿色小于等于 0.01，黄色在 0.01 到 0.03 之间，红色大于 0.03 触发复标定。

在口径 A 示例里，DataAnalysis 的 RR_gap 是 +0.0122，处于预警区间；logistic 是 +0.0326，已经进入需要复标定的区间；one-stage LLM 和 GPT-5.2 偏移更大，当前不具备主判稳定性。

因此我们采用“双榜机制”：主榜负责上线候选，扩展榜负责监控与校准设计。这样既不丢信息，也避免把高波动模型直接用于主决策。

过渡：基于以上结果，给出部署建议。

## Slide 8 部署建议与下一步（约 90 秒）
部署上建议分层：高风险客群采用 DataAnalysis + logistic 双线复核，全量预筛以 xgboost 作为主判，LLM 放在解释与复核辅助层，暂不建议直接主判。

同时明确三条风险边界：高风险子集不代表全客群；时间外推必须补测；标签和业务代价假设可能漂移。

执行路线按 30/60/90 天推进：30 天完成时间外滚动验证与阈值回放，60 天上线 RR_gap 自动告警，90 天启动结构化+文本联合建模试点。达到稳定性门槛后，再评估 LLM 主判可行性。

过渡：最后是可复现与交付闭环。

## Slide 9 可复现证据链与交付清单（约 70 秒）
我们把交付做成证据链：数据准备、训练、测试、报告固化，每个环节都有可审计产物，并可回查到 `run_report.json`。

提交前有四项硬检查：研究问题与结果证据一一对应；无测试集调参；关键结论能回查到原始报告文件；展示稿与书面报告叙述一致。

这保证了项目不仅“能讲”，更“能复现、能审计、能落地”。

过渡：最后给出总结。

## Slide 10 结论与 Q&A（约 60 秒）
最后总结四点：
第一，在固定拒绝率的公平比较下，结构化模型仍然是主判首选。第二，在全量真实分布下，xgboost 综合表现最优。第三，LLM 现阶段更适合解释和复核辅助层。第四，下一阶段优化优先级是“稳定性与校准”，再谈主判替代。

我也预留两个讨论问题：如果业务希望进一步提升召回，如何控制 RR 偏移；以及在什么条件下，LLM 可以从辅助层进入主判层。

谢谢大家，欢迎提问。
