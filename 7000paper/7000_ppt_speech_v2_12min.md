# 7000 PPT 演讲稿（12 分钟详讲版 v2）

> 对应文件：`7000paper/7000_presentation_v4.pptx`  
> 目标总时长：约 12 分钟（约 720 秒）  
> 建议语速：中文正常语速，关键数据处放慢半拍

## 时间分配总览
- Slide 1：35 秒
- Slide 2：70 秒
- Slide 3：80 秒
- Slide 4：70 秒
- Slide 5：95 秒
- Slide 6：85 秒
- Slide 7：90 秒
- Slide 8：85 秒
- Slide 9：65 秒
- Slide 10：45 秒

---

## Slide 1 封面（约 35 秒）
大家好，我是 `<姓名/学号>`。今天汇报的是我的 DSCI 7000 顶点项目，主题是“基于双口径评估的信贷风控比较”。

这次汇报我想解决一个很实际的问题：在信贷风控里，模型在“学术评估最优”和“真实上线最优”之间，经常不是同一个答案。我们这次做的不是只追一个分数，而是设计一套可复现、可解释、可部署的比较框架，最终输出可以落地的模型建议。

过渡：先明确本项目要回答的研究问题。

## Slide 2 研究问题与目录（约 70 秒）
本项目围绕四个问题推进。

第一，统一流程下，不同模型家族谁更稳健。这里的“统一流程”非常关键，因为不统一就无法公平比较。

第二，固定拒绝率约束会不会改变模型排序。也就是在业务代价相近时，谁真正更强。

第三，LLM 在当前阶段到底适合主判还是辅助。这个问题我们不预设立场，而是让结果说话。

第四，共享高风险子集上的结论，能否外推到全量分布。这是从“实验结论”走向“业务结论”的核心断点。

目录上我会按这四个问题展开：先讲双口径评估协议，再讲口径 A 主榜和口径 B 部署结果，然后说明监控和治理边界，最后给部署路线和交付闭环。

过渡：下面进入方法框架。

## Slide 3 双口径方法设计（约 80 秒）
我们采用双口径，但流程完全一致：`train_fit` 训练，`validation` 按目标 RR 定阈，`test` 固定阈值评估，最终输出 `run_report.json`。

我特别强调两条方法纪律。

第一，禁止测试集调参。任何阈值、策略都在验证集确定，测试集只做一次性评估。这是防止信息泄露，也是报告可信的底线。

第二，保持跨模型一致的评估管线。包含同样的数据切分原则、同样的阈值决策逻辑、同样的报告产物格式。这样后续比较才不是“方法差异+流程差异”的混合效应。

双口径的作用是“分而治之”：
- 口径 A 回答公平比较问题；
- 口径 B 回答上线表现问题。

后续你会看到，同一模型在两个口径里表现并不一致，这恰好说明双口径是必要的。

过渡：在这套协议下，先看模型候选池。

## Slide 4 模型家族与试点产物（约 70 秒）
我们放入四类模型家族。

第一类是 DataAnalysis 规则化基线，优势是强可解释、容易转成风控规则。

第二类是结构化机器学习模型，包含 logistic 和 xgboost，这两类通常是信贷场景的工程主力。

第三类是 BERT 文本模型，验证文本特征在当前数据结构下的增益。

第四类是 LLM 路线，包含 two-stage、one-stage 和 GPT-5.2 变体，用来评估大模型在风控链路中的角色边界。

试点通过标准有四条：数据一致、流程一致、结果可回查、偏差可解释。只有满足这四条，结果才进入主报告。这一点能避免“单次跑得好看但无法复现”的问题。

过渡：接下来先看口径 A，也就是公平主榜。

## Slide 5 口径 A 主结果（公平主榜，约 95 秒）
口径 A 的核心规则是：只比较 Reject Rate 接近 35% 的模型。这样我们在“差不多代价”下比较模型能力，避免模型靠更高拒绝率换更好分数。

在这个约束下，`logistic_tabular` 的 Recall 最高，为 0.4538，说明它在捕获坏样本上更积极。

`DataAnalysis` 的 Precision 略高，为 0.4175，说明它拒绝样本中的命中质量更好。

这两个结果对应两种业务偏好：
- 若更担心漏坏账，偏向 Recall 更高的方案；
- 若更担心误拒优质客户，偏向 Precision 更高的方案。

扩展组里，one-stage LLM 和 GPT-5.2 看上去召回更高，但伴随 RR 大幅偏移，因此不纳入主榜结论。这里的原则很简单：主榜要能支撑上线决策，不能由不可控代价驱动。

一句话总结这页：在公平约束下，结构化模型仍具备更稳定的主判价值。

过渡：那如果切换到真实全量分布，会发生什么？

## Slide 6 口径 B 结果与误差结构（约 85 秒）
在口径 B，也就是全量分布场景里，最优模型变为 `xgboost_tabular`，Precision 0.3153，Lift 2.0568。

这里 Lift 的含义是“相对基准坏账率的提升倍数”。Lift 超过 2，说明模型在识别高风险样本方面有显著增益。

重点不是这个数值本身，而是“排序变化”：口径 A 和口径 B 的最优模型不同。

这说明两点。
- 第一，样本分布变化会改变最优解；
- 第二，目标函数变化也会改变最优解。

所以报告里必须并列给出“公平横评结论”和“部署口径结论”。如果把二者混为一个结论，模型治理就会失真。

过渡：既然结论会随口径变化，就要有运行期监控框架。

## Slide 7 监控规则与双榜边界（约 90 秒）
我们在运行期用 `RR_gap = RR_actual - RR_target` 做稳定性监控，并按绝对值分层：
- 绿色：`<= 0.01`，稳定；
- 黄色：`(0.01, 0.03]`，预警；
- 红色：`> 0.03`，触发复标定。

口径 A 示例中，DataAnalysis 的 RR_gap 为 +0.0122，处于黄色区；logistic 为 +0.0326，进入红色边界；one-stage LLM 与 GPT-5.2 偏移更大，暂不满足主判稳定要求。

因此我们采用“双榜机制”：
- 主榜用于上线候选评审，只收 RR 接近目标值的模型；
- 扩展榜用于策略观察，报告全体模型并解释偏移原因。

这套机制的好处是：既不丢掉潜在改进方向，也不把高波动模型直接推向生产主链路。

过渡：基于以上结果，给出部署层面的具体建议。

## Slide 8 部署建议与下一步（约 85 秒）
部署建议分三层。

第一层，高风险客群：采用 DataAnalysis + logistic 双线复核，优先保证解释性和审计友好。

第二层，全量预筛：采用 xgboost 作为主判，利用其在真实分布下的综合优势。

第三层，LLM：定位为解释与复核辅助层，而不是当前阶段的主判引擎。

同时要守住三条边界：高风险子集不代表全客群；时序外推必须补测；标签和代价假设存在漂移风险。

落地节奏按 30/60/90 天推进：
- D+30 做时间外滚动验证与阈值回放；
- D+60 上线 RR_gap 自动监控告警；
- D+90 启动结构化+文本联合建模试点。

只有在稳定性达标后，才进入“LLM 是否主判”的下一轮评估。

过渡：最后说明这项工作如何保证可复现与可审计。

## Slide 9 可复现证据链与交付清单（约 65 秒）
我们的交付不是单一结论，而是证据链：从数据准备、模型训练、测试评估到报告固化，每个环节都有对应产物，可以逐项回查。

最终提交前我们设置了四项硬检查：
1) 研究问题与结果证据一一对应；
2) 全流程无测试集调参；
3) 报告中的关键结论可回查到 `run_report.json`；
4) 展示稿与书面报告叙述一致。

这四项检查解决的，是“结果可信”“过程透明”“审计可追溯”三个交付风险。

过渡：最后做一个收束总结。

## Slide 10 结论与 Q&A（约 45 秒）
总结四点。

第一，在固定拒绝率的公平比较下，结构化模型仍是主判首选。

第二，在全量真实分布下，xgboost 的综合效果更优。

第三，LLM 当前更适合作为解释与复核辅助层。

第四，下一阶段优先做稳定性与校准，再讨论主判替代。

以上是我的汇报，谢谢大家。欢迎围绕两个问题讨论：如果业务要更高召回，如何控制 RR 偏移；以及 LLM 在什么前提下可以进入主判层。

---

## 备选加长句（答辩追问时可用）
- 如果老师追问“为什么不用单一指标选模型”：可以回答“单一指标会掩盖代价结构，双口径是把公平比较与真实部署显式拆开，避免错误迁移”。
- 如果追问“LLM 是否没有价值”：可以回答“当前价值主要在解释增强、文本复核和策略提示，不是零价值，而是角色边界需要更谨慎定义”。
- 如果追问“下一步最关键实验”：可以回答“时间外滚动验证与阈值稳定性回放，这是上线前必须补齐的工程证据”。
