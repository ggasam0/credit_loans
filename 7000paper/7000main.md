# 基于双口径评估的信贷风控顶点项目：结构化模型与文本模型的可落地比较

作者：`<姓名/学号>`  
课程：DSCI 7000 Capstone  
日期：2026-02-21

## 摘要
本报告基于 LendingClub 历史数据构建信贷风险评估闭环，重点是可复现执行而非方法堆叠。研究采用双口径评估：口径 A 在共享高风险子集（`n=7108`）下固定拒绝率（`RR=35%`）进行公平横评，口径 B 在全量样本（`n=123,202`）下按实际违约率目标（`RR=0.1533`）进行部署评估。结果显示，口径 A 中 `DataAnalysis` 与 `ML(logistic_tabular)` 最稳健，口径 B 中 `ML(xgboost_tabular)` 综合最优。LLM 两阶段具备解释价值但主判性能仍弱，单阶段 LLM 与 GPT-5.2 主要依赖高拒绝率换取高召回，暂不满足公平主榜比较约束。

**关键词**: 顶点项目, 信用风险, LendingClub, 双口径评估, 固定拒绝率, 可复现性

## I. 引言
### A. 项目背景
消费信贷风险评估长期依赖结构化变量（利率、信用等级、收入、负债比）构建判别模型 [1], [2]。在 P2P 借贷场景中，借款描述等软信息也被证明具有增量价值 [3], [4]。随着 BERT 与 LLM 的发展 [5], [6]，文本建模能力显著提升，但在风控业务中，模型优劣不能只看离线分数，还必须满足阈值可控、拒绝率可管、误放损失可解释等运营约束。

### B. 报告定位与写作重点
本报告以项目执行质量为主线，强调“方案设计-实验评估-结果解释-展示落地”的闭环，并将结论映射到可验证的代码与数据产物。

### C. 本报告的实际贡献
1. 建立双口径评估框架，分离“公平横评”与“真实分布部署”结论。  
2. 给出跨模型家族（ML/DataAnalysis/BERT/LLM）的统一阈值流程与比较边界。  
3. 形成可审计的代码、数据与文档交付链。

## II. 研究范围与重点
本报告仅保留与结论直接相关的技术内容，重点分析统一阈值协议下的跨模型可比性，以及双口径排序差异的部署含义。

## III. 研究问题、目标与假说
### A. 研究问题
`RQ1`：在统一数据与阈值流程下，结构化模型、BERT、LLM 的主判表现如何比较？  
`RQ2`：固定拒绝率约束是否改变模型排序？  
`RQ3`：LLM 在当前工程条件下更适合主判还是辅助解释？  
`RQ4`：共享子集结论是否能外推到全量真实分布？

### B. 研究目标
1. 形成跨模型家族的公平比较基线。  
2. 形成“可横评 + 可部署参考”的双口径结果。  
3. 形成可直接用于项目展示的证据链。

### C. 可检验假说
`H1`：共享高风险子集上，结构化强信号模型将整体优于文本模型。  
`H2`：若缺乏拒绝率校准，LLM 可能通过高拒绝率获得高召回。  
`H3`：切换到全量真实分布后，模型排序将发生变化。

### D. 研究问题与评估维度映射
为避免“问题定义”和“结果展示”脱节，本项目将研究问题直接映射到可评分的评估维度：

1. `RQ1` 与 `H1` 对应“文献与方法完整性”维度：是否在统一流程下给出跨模型家族的可比结论。  
2. `RQ2` 与 `H2` 对应“实证质量”维度：是否明确拒绝率约束、阈值流程及公平比较边界。  
3. `RQ3` 对应“分析深度”维度：是否区分主判能力与解释能力，并给出可执行的角色分工。  
4. `RQ4` 与 `H3` 对应“结论外推边界”维度：是否通过双口径显示排序变化，而非将单一口径直接泛化。

该映射关系使后续章节形成一致结构：方法章节说明如何保证可比性，结果章节给出量化证据，讨论章节给出部署决策和边界说明。

## IV. 背景与范围界定
### A. 最小必要文献脉络
经典信用评分研究奠定了评估范式与业务指标体系 [1], [2]。P2P 研究显示软信息在信息不对称场景下可提供增量价值 [3], [4]。深度文本模型和大模型提升了语义理解能力 [5], [6]，但在金融场景中仍需遵循稳定性、可解释与合规约束 [7], [8]。

### B. 本项目范围与边界
本项目仅覆盖违约风险识别，不覆盖：

1. 反欺诈检测与身份核验。  
2. 贷后催收策略优化。  
3. 宏观压力测试与资产组合管理。

### C. 相关工作比较（按主题）
围绕本项目问题，可将相关研究分为三类，并与本项目定位形成对照：

1. **经典信用评分范式**：传统研究强调结构化变量与统计判别稳定性 [1], [2]。本项目继承该范式，在口径 A/B 中均以统一阈值流程进行比较，确保不同模型结果可对齐。  
2. **P2P 软信息利用路线**：既有研究证明借款描述可提供增量信息 [3], [4]。本项目并未假设文本必然优于结构化特征，而是把文本模型放在统一拒绝率约束下检验其真实边际贡献。  
3. **深度文本与大模型路线**：BERT 与参数高效微调方法提升了文本表示能力 [5], [6]，大模型带来更强语义解释能力 [7]。本项目结论是：解释能力与主判能力需拆分评估，避免将“可解释”直接等同于“可替代主判”。

该主题化比较对应本报告的核心定位：不是再做一次方法罗列，而是在统一业务约束下回答“哪些方法在当前场景可部署、哪些仅适合作为辅助层”。

### D. 既有研究局限与本项目回应
在综合 [1]-[8] 的研究脉络后，可识别出四类与工程落地直接相关的常见局限：

1. 很多研究以通用分类指标为主，较少在固定拒绝率约束下进行公平比较。  
2. 软信息研究常集中于特定数据切片，结论对全量业务分布的外推边界不够明确。  
3. 深度文本或大模型研究强调语义能力，但对阈值稳定性与拒绝率偏移讨论不足。  
4. 缺少跨模型家族、同协议下的统一比较框架，容易出现“样本不一致比较”。

本项目对应的改进策略如下：

| 既有局限 | 本项目回应 | 证据位置 |
|---|---|---|
| 指标侧重通用分类，缺少拒绝率约束 | 采用双口径 + 目标拒绝率评估协议 | `V.C`, `VII.B`, `VII.J` |
| 结论依赖局部样本，外推不清晰 | 同时报告共享子集与全量分布结果 | `V.A`, `VII.A`, `VII.C`, `VII.I` |
| 文本模型解释与主判角色混淆 | 区分主判层与辅助解释层职责 | `VII.K`, `VIII.C`, `IX` |
| 跨家族比较口径不统一 | 统一切分、统一阈值流程、统一指标定义 | `V.E`, `V.F`, `VI.B` |

这种“局限-回应-证据”的组织方式，确保文献综述不止于描述已有工作，而是明确本项目如何在现有工程条件下补足关键缺口。

## V. 方法与实现方案
### A. 数据与口径设计
原始数据来自 LendingClub 历史公开数据 [9]。清洗后可用样本 `123,202` 条；共享高风险子集 `7,108` 条。

两套口径：

1. **口径 A（公平横评）**：`data/shared/shared_subset.csv`，`train_fit=4548`，`validation=1138`，`test=1422`，目标 `RR=35%`。  
2. **口径 B（业务分布）**：`ml_full_processed.csv`，`train_fit=78848`，`validation=19713`，`test=24641`，目标 `RR=0.15327673252057597`。

### B. 模型族与实现路径
| 模型族 | 代表脚本 | 说明 |
|---|---|---|
| ML | `ml_standalone/run_ml_pipeline.py` | Logistic / XGBoost |
| DataAnalysis | `data_analysis_standalone/run_data_analysis_classifier.py` | 规则化风险画像基线 |
| BERT | `bert_standalone/run_bert_pipeline.py` | Embedding + Finetune |
| LLM 两阶段 | `llm_standalone/run_llm_two_stage.py` | Reason -> Action |
| LLM 单阶段 | `llm_standalone/run_llm_one_stage.py` | Direct Action |

### C. 指标与阈值协议
以“拒绝”为正类，使用以下业务指标：

- `Precision(reject)`：被拒样本中真实违约比例。  
- `Recall(default)`：违约捕获率。  
- `Approval Bad Rate`：放行样本坏账率。  
- `Reject Rate`：拒绝比例。  
- `Lift`：相对随机拒绝提升。

统一阈值规则：验证集定阈值，测试集仅评估；禁止测试集调参。

### D. 可复现配置快照
关键复现实验设置如下：

1. 两口径均采用分层随机切分（`random_state=42`），并执行“验证集定阈、测试集固定评估”。  
2. LLM 两阶段采用 `Qwen3-0.6B + LoRA`（`lora_rank=8`, `learning_rate=1e-4`）；BERT 有效输入长度 `512`。  
3. ML 路线统一使用共享特征选择结果与同一切分文件，避免样本不一致比较。

### E. 训练与评估执行流程
为保证实验可复查，所有模型遵循同一执行顺序：

1. 在 `train_fit` 上训练模型参数，不访问测试集标签。  
2. 在 `validation` 上通过目标拒绝率确定阈值，并冻结阈值配置。  
3. 在 `test` 上一次性评估 `Precision(reject)`、`Recall(default)`、`Approval Bad Rate`、`Lift` 与 `Reject Rate`。  
4. 将关键输出写入对应 `run_report.json` 与分模型报告文档，保证“代码结果-报告结论”一致。

该流程的核心约束是“测试集只评估不调参”，用于避免结果乐观偏差，并确保跨模型比较具有可解释性。

### F. 证据链与可审计保障
基于 research-planning 思路，本项目将“数据-训练-评估-报告”组织为可回溯证据链，避免口头结论与落地产物脱节：

| 环节 | 关键输入 | 关键输出 | 审计检查点 |
|---|---|---|---|
| 数据准备 | `data/shared/shared_subset.csv`, `ml_full_processed.csv` | 分层切分文件 | 样本规模、标签分布与口径一致性 |
| 模型训练 | 各模型 `run_*.py` 脚本 | 模型预测结果与阈值配置 | 是否仅使用 `train_fit/validation` 定阈 |
| 测试评估 | 固定测试集与统一指标协议 | `run_report.json` | 是否存在测试集反向调参 |
| 报告固化 | 主文档与分模型报告 | 文内表格与结论 | 指标数值与 JSON 文件一致 |

该机制的价值在于：任何结论都可以被定位到“对应脚本 + 对应数据 + 对应指标文件”，从而支持复评、答辩和后续维护。

## VI. 试点与数据筛选
### A. 试点目标
本阶段核心目标是验证“数据筛选 + 切分 + 指标流程”是否可稳定复现，为后续批量实验提供基线。

### B. 试点输出
1. 共享切分目录：`data/shared/splits/stratified`。  
2. 全量切分目录：`data/shared/splits/full_actual_rr`。  
3. 各模型可读取同一 `_shared_row_id` 对齐样本，避免隐性样本偏差。

### C. 试点通过标准（Pilot Exit Criteria）
为避免“试点已完成但不可扩展”的问题，本项目定义以下最小通过标准：

1. **数据一致性通过**：两口径样本规模、标签口径与切分比例可复现。  
2. **流程一致性通过**：所有模型均遵循“训练-验证定阈-测试评估”的统一流程。  
3. **结果一致性通过**：主文档关键指标可回查到对应 `run_report.json`。  
4. **偏差可解释通过**：拒绝率偏差（`RR_gap`）和坏账率差异可在报告中解释。  

当以上四项全部满足时，试点阶段才进入“批量实验与展示材料固化”阶段。

## VII. 数据分析与结果
### A. 口径 A：共享子集（`n=7108`, `RR=35%`）
| 模型 | Precision(reject) | Recall(default) | Approval Bad Rate | Lift | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML (logistic_tabular) | 0.4154 | 0.4538 | 0.3098 | 1.1870 | 0.3826 |
| ML (logistic_text_fusion) | 0.4154 | 0.4337 | 0.3126 | 1.1868 | 0.3657 |
| ML (xgboost_tabular) | 0.4030 | 0.4378 | 0.3178 | 1.1513 | 0.3805 |
| DataAnalysis | 0.4175 | 0.4317 | 0.3120 | 1.1928 | 0.3622 |
| BERT-Embedding | 0.3859 | 0.4177 | 0.3284 | 1.1026 | 0.3790 |
| BERT-Finetune | 0.3895 | 0.4317 | 0.3253 | 1.1128 | 0.3882 |
| LLM 0.6B 两阶段微调 | 0.3653 | 0.3675 | 0.3420 | 1.0436 | 0.3523 |
| LLM 0.6B 单阶段微调 | 0.3587 | 0.6245 | 0.3369 | 1.0249 | 0.6097 |
| GPT-5.2 未微调 | 0.3462 | 0.8133 | 0.3690 | 0.9884 | 0.8228 |

### B. 公平比较边界
在口径 A 中，仅拒绝率接近 35% 的模型可进入公平主榜（前 7 条）。单阶段 LLM 和 GPT-5.2 因拒绝率显著偏高，不参与主榜优劣结论。

### C. 口径 B：全量业务分布（`n=123,202`, `RR=0.1533`）
| 模型 | Precision(reject) | Recall(default) | Approval Bad Rate | Lift | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML (logistic_tabular) | 0.3069 | 0.2992 | 0.1263 | 2.0023 | 0.1494 |
| ML (logistic_text_fusion) | 0.2994 | 0.3050 | 0.1262 | 1.9532 | 0.1562 |
| ML (xgboost_tabular) | 0.3153 | 0.3095 | 0.1246 | 2.0568 | 0.1505 |
| DataAnalysis | 0.2849 | 0.2851 | 0.1294 | 1.8589 | 0.1534 |

### D. 假说检验结论
1. `H1` 成立：共享高风险口径下结构化路线更稳健。  
2. `H2` 成立：部分 LLM 路线存在“高拒绝率换高召回”。  
3. `H3` 成立：全量口径模型排序变化，`xgboost_tabular` 成为最优。

### E. 关键差异的量化解读
1. 在口径 A 中，`DataAnalysis` 相比两阶段 LLM 的 `Precision(reject)` 提升 `0.0522`（`0.4175 - 0.3653`），对应约 `14.3%` 的相对提升。  
2. 在口径 A 中，`ML(logistic_tabular)` 相比 `DataAnalysis` 的 `Recall(default)` 提升 `0.0221`，但 `Precision(reject)` 略低 `0.0021`，说明二者更像“精度-召回”权衡关系而非绝对替代。  
3. 在口径 B 中，`ML(xgboost_tabular)` 相比 `DataAnalysis` 的 `Precision(reject)` 提升 `0.0304`，`Lift` 提升 `0.1979`，表明全量分布下树模型优势更明显。  
4. 单阶段 LLM 与 GPT-5.2 的高召回伴随显著更高拒绝率（`0.6097`、`0.8228`），改进主要来自策略变化而非同口径判别质量提升。

### F. 混淆矩阵视角（核心模型）
为避免仅凭单一比例指标解读，本节给出核心模型在测试集上的 `TP/FP/TN/FN` 计数。

口径 A（`n=1422`）：

| 模型 | TP | FP | TN | FN | Reject Rate |
|---|---:|---:|---:|---:|---:|
| DataAnalysis | 215 | 300 | 624 | 283 | 0.3622 |
| ML (logistic_tabular) | 226 | 318 | 606 | 272 | 0.3826 |
| BERT-Finetune | 215 | 337 | 587 | 283 | 0.3882 |
| LLM 两阶段 | 183 | 318 | 606 | 315 | 0.3523 |
| LLM 单阶段 | 311 | 556 | 368 | 187 | 0.6097 |
| GPT-5.2 | 405 | 765 | 159 | 93 | 0.8228 |

口径 B（`n=24641`）：

| 模型 | TP | FP | TN | FN | Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML (xgboost_tabular) | 1169 | 2539 | 18325 | 2608 | 0.1505 |
| ML (logistic_tabular) | 1130 | 2552 | 18312 | 2647 | 0.1494 |
| DataAnalysis | 1077 | 2703 | 18161 | 2700 | 0.1534 |

### G. 每千笔业务代价解释
将混淆矩阵折算为“每 1000 笔申请”后，结论更直观：

1. 口径 A：`ML(logistic_tabular)` 约捕获违约 `158.9`、放过违约 `191.3`、误拒好样本 `223.6`；`DataAnalysis` 为 `151.2/199.0/211.0`；`LLM 两阶段` 为 `128.7/221.5/223.6`。  
2. 口径 B：`ML(xgboost_tabular)` 为 `47.4/105.8/103.0`，优于 `DataAnalysis` 的 `43.7/109.6/109.7`。

### H. 阈值稳定性与上线监控建议
定义 `RR_gap = RR_actual - RR_target` 用于监控阈值偏移。  
口径 A 中，`DataAnalysis` 为 `+0.0122`、`ML(logistic_tabular)` 为 `+0.0326`、`LLM 单阶段` 为 `+0.2597`、`GPT-5.2` 为 `+0.4728`；口径 B 中，`ML(xgboost_tabular)` 为 `-0.0028`、`ML(logistic_tabular)` 为 `-0.0039`、`DataAnalysis` 为 `+0.0001`。

建议：

1. `|RR_gap| <= 0.01` 视为稳定，`0.01 < |RR_gap| <= 0.03` 预警，`|RR_gap| > 0.03` 触发复标定。  
2. 与 `RR_gap` 同步监控 `Approval Bad Rate`；高波动模型先作为辅助意见流，不直接主判。  

### I. 口径间排序变化的机制解释
从结果看，口径 A 与口径 B 的最佳模型不同，这不是冲突，而是样本分布与目标函数共同作用的结果：

1. 口径 A 为高风险筛选子集，样本已具备更强结构化风险信号，文本特征的边际增益被压缩。  
2. 口径 B 覆盖全量业务分布，类别结构更接近真实投产环境，树模型对非线性与交互特征的利用更充分。  
3. 固定拒绝率与实际违约率目标分别代表“公平横评”与“部署约束”两类问题，应分别给出结论而非混合解释。  
4. 因此，本报告将模型选择分为两层：离线横评层看同口径判别质量，部署决策层看目标拒绝率稳定性与坏账控制表现。

### J. 主榜与扩展榜的双层报告规则
为减少“单一排行榜”导致的误读，本项目采用主榜与扩展榜并行汇报：

1. **主榜（公平比较）**：仅纳入拒绝率接近目标值的模型，用于回答“同等业务约束下谁更优”。  
2. **扩展榜（策略观察）**：纳入所有模型，用于展示“放宽约束后模型会如何改变召回与拒绝行为”。  

以口径 A 为例，单阶段 LLM 与 GPT-5.2 的 `Recall(default)` 虽高，但 `Reject Rate` 分别达到 `0.6097` 和 `0.8228`，显著偏离 `RR=0.35` 约束，因此不应与主榜模型直接比较优劣。该规则的作用是把“判别能力”与“策略激进度”分离解释，避免在展示中把高召回误读为可直接上线。

在答辩或业务汇报中，建议始终同时报告三项信息：目标拒绝率、实际拒绝率偏差、同口径下的 Precision/Lift 排序。只有三者同时满足时，模型才进入部署候选名单。

### K. 模型家族误差模式与适用边界
结合口径 A/B 结果与混淆矩阵，可将模型家族的误差模式概括为下表：

| 模型家族 | 主要优势 | 主要风险 | 当前建议角色 |
|---|---|---|---|
| ML（Logistic） | 规则清晰、阈值可解释、在公平口径下稳定 | 复杂非线性关系捕捉能力有限 | 主判候选（高可控场景） |
| ML（XGBoost） | 全量分布下 Precision/Lift 优势明显 | 解释成本相对更高 | 主判候选（自动预筛场景） |
| DataAnalysis | 业务可读性强、基线稳健 | 上限受规则表达能力限制 | 基线与复核支撑 |
| BERT | 文本表示能力优于传统特征工程 | 在当前口径下边际收益不稳定 | 文本增强探索 |
| LLM（两阶段/单阶段） | 解释信息丰富、可支持人工复核 | 拒绝率与决策稳定性波动较大 | 辅助解释层（暂不主判） |

该对照强调“模型优劣与使用场景绑定”：同一模型可在某口径表现突出，但在另一口径可能因约束条件变化而不再占优。

### L. 指标冲突下的模型选择准则
在实际评估中，`Recall(default)`、`Reject Rate`、`Approval Bad Rate` 往往不会同时最优，因此需要预先定义冲突处理顺序。基于当前结果，建议采用如下优先级：

1. **第一层（硬约束）**：先检查 `RR_actual` 是否满足目标拒绝率区间；不满足者仅进入扩展榜观察。  
2. **第二层（风险约束）**：在满足拒绝率约束的模型中，优先最小化 `Approval Bad Rate`。  
3. **第三层（增益约束）**：在前两层通过后，比较 `Precision(reject)` 与 `Lift`，选取相对随机策略收益更高者。  
4. **第四层（运营约束）**：若指标接近，则优先选择解释与复核成本更低、阈值更稳定的方案。

可将该规则简化为下表用于汇报现场快速决策：

| 冲突场景 | 推荐判断顺序 | 典型处理 |
|---|---|---|
| 高召回但拒绝率过高 | 先看 `RR_actual` 是否越界 | 越界模型不入主榜，仅作策略参考 |
| 坏账率更低但增益接近 | 先看 `Approval Bad Rate` 再看 `Lift` | 优先坏账更低者 |
| 指标相近、模型复杂度不同 | 加入稳定性与解释成本 | 选阈值更稳、治理成本更低者 |

该准则的目的不是替代实验指标，而是确保在“指标不一致”时仍能给出一致、可复核的选择结论。

## VIII. 讨论与部署启示
### A. 结果解释
1. 共享子集中的结构化信号浓度高，压缩了文本模型边际增益。  
2. LLM 当前优势更偏解释能力，而非稳定主判能力。  
3. 在固定拒绝率约束下，`DataAnalysis` 与 `ML(logistic_tabular)` 的表现更接近“可控优化”，即在保持可解释阈值策略的同时提供稳定 Precision/Lift。  
4. 在全量分布口径中，`ML(xgboost_tabular)` 优势更明显，说明非线性与特征交互在真实客群中更重要。  
5. 双口径结果提示：单一数据口径结论不宜直接外推，模型结论必须绑定具体目标函数与业务约束。

上述解释可转化为一条操作性规则：先用公平口径判定“谁在同约束下更优”，再用全量口径验证“该优选模型在真实分布下是否仍稳定”。只有两步均通过，才进入上线候选。

### B. 项目执行反思
1. 先定义公平比较边界，再解释模型差异。  
2. 统一切分与阈值流程，是避免比较失真的关键。  
3. 结果管理必须“先日志、后叙述”，即先固化 `run_report.json` 再写主文档结论。  
4. 对高波动模型应先做辅助层定位，再讨论是否进入主判，避免在评估阶段过度承诺。  
5. 双口径设计虽然增加工作量，但显著降低了“单口径误判上线策略”的风险。

从项目管理角度看，本项目最关键的经验是把“研究问题-评估协议-产物交付”绑定在同一链路中：任何结论都必须能回溯到对应脚本、对应切分和对应指标文件。

### C. 决策建议（按场景）
| 场景 | 推荐方案 | 主要依据 | 当前限制 |
|---|---|---|---|
| 高风险客群受控审批 | `DataAnalysis + ML(logistic_tabular)` 双线复核 | 口径 A 下精度与召回更均衡 | 需人工复核流程配合 |
| 全量分布自动预筛 | `ML(xgboost_tabular)` 主判 | 口径 B 下 Precision/Lift 最优 | 需要持续监控漂移 |
| 文本解释与复核辅助 | LLM 两阶段 | 可提供可读理由与补充语义 | 主判性能仍弱于结构化基线 |
| 大模型直接主判 | 当前不建议 | 单阶段/GPT-5.2 拒绝率偏高 | 需先完成 RR 严格校准 |

### D. 结果传播与受众沟通
面向学术评审强调方法严谨性与证据链完整性，面向业务听众强调拒绝率约束下的模型选择与上线风险。

为避免同一结果被不同受众误读，建议使用“双版本表达”：

1. **学术版表达**：突出问题设定、比较边界、假说检验与可复现性。  
2. **业务版表达**：突出拒绝率目标、坏账控制、误拒代价与监控阈值。  

建议在汇报页固定保留一张“共识页”，包含三项最小信息：`RR_target`、`RR_actual`、`Approval Bad Rate`。若任一缺失，听众容易把“高召回”误读为“高收益”。

### E. 展示实施建议
展示材料建议采用“问题-证据-决策”结构，并控制在 12-15 页：

1. 问题定义与研究目标（1-2 页）：明确 `RQ/H` 与业务目标的对应关系。  
2. 数据与评估协议（2-3 页）：说明两口径、切分方式与阈值规则，先讲比较边界再讲结果。  
3. 核心结果与对比（4-5 页）：按“主榜 -> 扩展榜 -> 混淆矩阵 -> 每千笔代价”顺序展示。  
4. 部署建议与风险控制（2-3 页）：展示模型分层使用策略与 `RR_gap` 监控方案。  
5. 局限与后续路线（1-2 页）：给出优先级与验收标准，避免“只有建议没有落地”。

每页建议固定保留最小上下文：数据口径、样本规模、目标拒绝率、关键指标。这样可减少听众在翻页时丢失比较条件。

问答准备建议聚焦四类高频问题：

1. 为什么高召回不等于可上线最优？  
2. 为什么口径 A 与口径 B 排序不同？  
3. 为什么当前将 LLM 放在辅助层而非主判层？  
4. 如何验证上线后拒绝率与坏账率不会快速漂移？

时间控制建议：前半段（问题与方法）控制在总时长 40%，后半段（结果、部署与局限）控制在 60%，确保核心证据和决策建议讲透。

### F. 有效性威胁与边界
1. 共享子集是高风险筛选样本，结论不能直接代表全量客群。  
2. 当前切分以分层随机为主，尚未完成严格时序外推检验。  
3. 当前标签口径以历史违约结果为准，可能受到经济周期与平台策略变化影响。  
4. 每千笔代价换算基于静态业务假设，真实投产中会随客群与策略调整发生漂移。  
5. LLM 路线存在阈值校准与决策稳定性问题，现阶段结论更偏“可用性评估”而非“能力上限”。  

上述威胁可归纳为三类偏差风险：样本代表性偏差、阈值稳定性偏差、业务代价估计偏差。为便于部署前审阅，建议将威胁与指标监控一一对应：

| 威胁类型 | 主要影响指标 | 潜在偏差方向 | 建议监控信号 |
|---|---|---|---|
| 样本代表性不足 | Precision/Lift 排序 | 线下排序高估或低估 | 分客群分层指标差异、时间切片表现 |
| 阈值外推不足 | Reject Rate, RR_gap | 上线后拒绝率偏移 | `RR_gap` 日/周趋势、阈值漂移日志 |
| 标签口径时变 | Recall(default) | 违约捕获率失真 | 新账期回溯验证、口径一致性检查 |
| 代价假设静态化 | 每千笔代价 | 风险收益评估偏差 | 客群结构变动、审批策略版本对比 |
| LLM 决策波动 | Reject Rate, Approval Bad Rate | 决策不稳定 | 多批次重跑方差、提示词版本追踪 |

### G. 局限分层与改进优先级
为避免“问题过多、改进无序”，可按影响程度分三层推进：

1. **高优先级（直接影响上线）**：补充时间外测试与滚动窗口评估，验证阈值和拒绝率在时序变化下的稳定性。  
2. **中优先级（影响模型上限）**：在统一拒绝率约束下开展文本增强与特征融合实验，观察文本信息是否在全量分布中提供稳定增益。  
3. **低优先级（影响解释与治理）**：完善 LLM 辅助解释模板与人工复核规范，提升审计可读性和复盘效率。

该分层策略的目标是先保证部署安全，再追求性能增益，最后提升治理与协同效率。

为避免“有优先级但无落地”，每层建议绑定最小验收条件：

1. **高优先级验收**：时间外测试中 `RR_gap` 与 `Approval Bad Rate` 波动保持在预设阈值内，且关键模型排序不出现反转。  
2. **中优先级验收**：在同一拒绝率约束下，文本增强或融合方案相对基线取得稳定增益，并能在重复实验中复现。  
3. **低优先级验收**：解释模板可被人工复核流程稳定使用，审计记录具备可追溯字段（模型版本、阈值版本、提示词版本）。

### H. 后续实验路线（四阶段）
结合当前代码与数据资产，后续实验可按“先稳再强”的四阶段推进：

1. **阶段 1（可运行基线）**：固定现有最佳配置，复跑口径 A/B 全流程，确认日志、报告与主文档指标一致。  
2. **阶段 2（基线调优）**：仅调整学习率、阈值搜索粒度与采样策略，不改模型结构，观察 `RR_gap` 与 `Approval Bad Rate` 的稳定性。  
3. **阶段 3（方法增强）**：在统一拒绝率约束下测试文本增强或结构化-文本融合方案，重点评估是否在口径 B 产生稳定增益。  
4. **阶段 4（消融与敏感性）**：对关键组件逐项移除或替换，报告多次运行均值与波动，验证结论是否依赖单一配置。

各阶段共同约束为：测试集不参与调参，且所有新增结论必须在对应 `run_report.json` 与分模型报告中可追溯。

### I. 交付就绪检查清单
在最终提交前，建议执行一次“研究-文档-展示”一致性检查：

| 检查项 | 通过条件 | 对应产物 |
|---|---|---|
| 研究问题与假说 | `RQ/H` 在结果章节有直接证据 | `III`, `VII.D` |
| 实证方法规范性 | 无测试集调参，阈值流程一致 | `V.E`, `V.F`, 脚本日志 |
| 结果可复核性 | 主表指标可映射到 JSON 文件 | `VII` + `output/*/run_report.json` |
| 讨论与边界完整性 | 局限、风险、改进路径齐备 | `VIII.F`, `VIII.G`, `VIII.H` |
| 展示可执行性 | 有清晰讲解顺序与问答准备 | `VIII.D`, `VIII.E` |

该清单用于确保书面报告、代码产物与最终展示三者叙述一致，避免“文档正确但无法复现”或“结果存在但无法讲清”。

## IX. 结论
本项目完成了完整研究闭环：问题定义、文献支撑、方案设计、试点筛选、数据分析、报告固化与展示实施。核心结论如下：

1. 在固定拒绝率公平约束下，结构化模型仍是当前最稳健主判路线。  
2. 在全量真实分布口径下，`xgboost_tabular` 综合表现最优。  
3. LLM 当前阶段更适合作为解释和复核辅助层，需先完成拒绝率校准后再参与主榜比较。

为便于从“研究结论”过渡到“上线决策”，建议补充四条落地判定条件：

1. **比较条件一致**：仅在统一口径、统一切分、统一阈值流程下比较主判模型。  
2. **阈值稳定达标**：`RR_gap` 持续处于可接受区间（优先 `|RR_gap| <= 0.01`，超出则触发复标定）。  
3. **业务风险可控**：在目标拒绝率约束下，`Approval Bad Rate` 与每千笔代价不劣于当前生产基线。  
4. **证据链可回溯**：所有结论都可回溯至对应脚本、数据切分和 `run_report.json` 指标文件。

若上述条件无法同时满足，则维持“结构化主判 + LLM 辅助解释”的分层架构，不进入大模型直接主判。

## 参考文献
[1] D. J. Hand and W. E. Henley, “Statistical Classification Methods in Consumer Credit Scoring: A Review,” *JRSS A*, vol. 160, no. 3, pp. 523-541, 1997, doi: 10.1111/j.1467-985X.1997.00078.x.  
[2] L. C. Thomas, “A survey of credit and behavioural scoring,” *International Journal of Forecasting*, vol. 16, no. 2, pp. 149-172, 2000, doi: 10.1016/S0169-2070(00)00034-0.  
[3] G. Dorfleitner *et al*., “Description-text related soft information in peer-to-peer lending,” *Journal of Banking & Finance*, vol. 64, pp. 169-187, 2016, doi: 10.1016/j.jbankfin.2015.11.009.  
[4] C. Jiang *et al*., “Loan default prediction by combining soft information extracted from descriptive text,” *Annals of Operations Research*, vol. 266, pp. 511-529, 2018, doi: 10.1007/s10479-017-2668-z.  
[5] J. Devlin *et al*., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” in *NAACL-HLT*, 2019, pp. 4171-4186, doi: 10.18653/v1/N19-1423.  
[6] E. J. Hu *et al*., “LoRA: Low-Rank Adaptation of Large Language Models,” arXiv:2106.09685, 2021, doi: 10.48550/arXiv.2106.09685.  
[7] OpenAI *et al*., “GPT-4 Technical Report,” arXiv:2303.08774, 2023, doi: 10.48550/arXiv.2303.08774.  
[8] T. Berg *et al*., “On the Rise of FinTechs: Credit Scoring Using Digital Footprints,” *Review of Financial Studies*, vol. 33, no. 7, pp. 2845-2897, 2020, doi: 10.1093/rfs/hhz099.  
[9] LendingClub, “LendingClub Statistics and Loan Data,” [Online]. Available: https://www.lendingclub.com/info/download-data.action. [Accessed: Feb. 21, 2026].

## 附录 A：关键产物
- 总报告与说明：`REPORT.md`, `README.md`
- 分模型报告：`ml_standalone/REPORT_ML.md`, `data_analysis_standalone/REPORT_DATA_ANALYSIS.md`, `bert_standalone/REPORT_BERT.md`, `llm_standalone/REPORT_LLM.md`
- 核心结果：`output/ml_standalone/run_report.json`, `output/ml_standalone_full_actual_rr/run_report.json`, `output/data_analysis_standalone_full_actual_rr/run_report.json`
- LLM 推理结果：`output/llm_two_stage_subset_full_06b/infer/stage2_action_predictions_full.json`, `output/llm_one_stage_subset_full_06b/infer/stage_action_direct_predictions_full.json`, `output/llm_two_stage_subset_full_06b/infer/gpt52_external_metrics.json`
