# DSCI 7000 顶点项目报告（基于 LendingClub 信贷风险评估项目）

作者：`<姓名/学号>`  
课程：DSCI 7000 Capstone  
日期：2026-02-21

## 摘要
本报告基于课程顶点项目要求，围绕“在真实业务约束下评估结构化模型与文本模型的风控价值”开展完整研究闭环。与 6800 课程侧重“文献综述 + 方法比较”不同，本报告重点放在**项目化执行过程**：研究问题与假说定义、分周任务推进、数据筛选与试点验证、模型开发与评估、结果解释、成果传播与最终展示。项目使用 LendingClub 历史数据构建统一实验框架，在共享子集口径（`n=7108`, `RR=35%`）与全量业务口径（`n=123,202`, `RR=0.1533`）下进行对比。结果显示：在拒绝率受控条件下，`DataAnalysis` 与 `ML(logistic_tabular)` 在共享口径最稳健；在全量真实分布下，`ML(xgboost_tabular)` 综合最优；LLM 两阶段具备可解释优势但判别力仍落后，单阶段与外部 GPT-5.2 当前存在拒绝率偏高问题。报告最终给出课程评分项对应证据与第 8 周展示方案，确保“研究-实现-评估-传播”全链路可复现。

**关键词**：Capstone, Credit Risk, Project-based Research, Fixed Reject Rate, Reproducibility, Research-to-Delivery

## 1. 项目定位与 6800 区分
本项目与 6800 的核心区别如下：

1. 6800 重点是“领域文献与方法比较”；7000 重点是“顶点项目全过程管理与交付”。  
2. 6800 更关注论文式分析深度；7000 更关注研究问题落地、里程碑执行、成果传播和最终展示。  
3. 7000 需要将课程安排（第 1-8 周）与实际产物（数据、代码、图表、报告）逐项对齐。

因此，本报告不再展开大规模文献枚举，而采用“课程目标 -> 项目执行 -> 产物证据 -> 展示交付”的组织方式。

## 2. 课程要求映射（评分与工时）
### 2.1 工时分配（课程建议）
| 活动类型 | 预估时长 | 占比 |
|---|---:|---:|
| 项目规划与提案撰写 | 10-15 小时 | 10%-15% |
| 独立研究与数据收集 | 25-30 小时 | 25%-30% |
| 项目开发（分析/设计/实施） | 30-35 小时 | 30%-35% |
| 导师沟通与协作 | 10-15 小时 | 10%-15% |
| 最终报告/展示材料整理 | 20-25 小时 | 20%-25% |
| 修改与最终提交 | 5-10 小时 | 5%-10% |

本项目执行采用上述分配作为计划基线，并在周计划中落实具体产物。

### 2.2 评分项映射
| 评分项 | 权重 | 本项目对应证据 |
|---|---:|---|
| 研究问题、目标与假说 + 文献综述 + 研究方案与工具设计 | 30% | 第 3 节、第 4 节、第 5 节；`README.md`、各模块 `README/REPORT` |
| 试点研究或数据筛选 + 数据分析 + 报告撰写 | 30% | 第 6 节、第 7 节；`output/*/run_report.json` 与结果表 |
| 最终展示 | 40% | 第 9 节（展示脚本、图表、传播与问答准备） |

## 3. 研究问题、目标与假说（Week 1 交付）
### 3.1 研究问题
`RQ1`：在统一数据与阈值流程下，结构化模型、BERT、LLM 的主判性能差异如何？  
`RQ2`：固定拒绝率约束是否改变模型优劣排序？  
`RQ3`：文本模型（尤其 LLM）在当前项目阶段更适合“主判”还是“辅助解释”？  
`RQ4`：在全量真实分布口径下，结论是否保持一致？

### 3.2 研究目标
1. 建立跨模型家族可公平比较的评估框架。  
2. 形成两套业务可解释口径：共享子集（公平横评）与全量分布（部署参考）。  
3. 产出可复现代码与可展示结论，满足顶点课程最终汇报需求。

### 3.3 可检验假说
`H1`：共享子集（高风险样本）下，结构化强信号模型将优于文本模型主判表现。  
`H2`：LLM 若未严格对齐目标拒绝率，可能通过高拒绝率换取高召回。  
`H3`：全量口径会导致模型内部排序变化（例如 Logistic 与 XGBoost 的相对位置变化）。

## 4. 文献与背景（Week 2，服务于问题界定）
本项目保留“最小必要文献”支撑，而非扩展性综述。核心共识：

1. 信用评分长期依赖结构化特征，评估协议的一致性决定结论可信度。  
2. P2P 场景存在信息不对称，文本软信息具有增量潜力。  
3. LLM 提升语义理解与解释能力，但在风控业务中仍需校准与阈值控制。

在 7000 课程语境下，文献的主要作用是支撑研究问题与实验设计，而不是成为报告主体。

## 5. 研究方案与工具设计（Week 3）
### 5.1 总体方案
本项目采用“统一数据 -> 统一切分 -> 验证集定阈 -> 测试集评估”的流程，避免测试集调参。

### 5.2 工具链与代码结构
| 模块 | 主脚本 | 作用 |
|---|---|---|
| ML | `ml_standalone/run_ml_pipeline.py` | Logistic/XGBoost 训练与评估 |
| DataAnalysis | `data_analysis_standalone/run_data_analysis_classifier.py` | 规则化风险画像基线 |
| BERT | `bert_standalone/run_bert_pipeline.py` | Embedding/Fine-tune 两路线 |
| LLM 两阶段 | `llm_standalone/run_llm_two_stage.py` | Reason->Action |
| LLM 单阶段 | `llm_standalone/run_llm_one_stage.py` | Direct Action |
| 公共流程 | `workflow_common.py` | 切分、阈值、业务指标统一 |

### 5.3 评估指标（业务解释导向）
以“拒绝”为正类：
- `Precision(reject)`：被拒样本中真实违约占比。  
- `Recall(default)`：违约捕获率。  
- `Approval Bad Rate`：放行样本中坏账率。  
- `Reject Rate`：拒绝比例。  
- `Lift`：相对随机拒绝的提升倍数。

## 6. 试点研究与数据筛选（Week 4）
### 6.1 数据筛选流程
原始数据 `2,260,701` 条，经清洗后得到 `123,202` 条可用样本。  
共享高风险子集筛选规则（用于公平横评）：
- `grade in {E, F, G}`  
- `int_rate >= 18.5`  
- `annual_inc <= 92,500`

最终共享子集：`n=7108`，分层切分为 `train_fit=4548`, `validation=1138`, `test=1422`。

### 6.2 两套评估口径
1. **口径 A（公平横评）**：`data/shared/shared_subset.csv`，目标 `RR=35%`。  
2. **口径 B（业务分布）**：`ml_full_processed.csv`，目标 `RR=0.15327673252057597`（真实违约率）。

该设计对应课程“试点/筛选”要求：先在可控子集统一比较，再回到全量分布做部署参考。

## 7. 数据分析与实验结果（Week 5-6）
### 7.1 口径 A：共享子集（`n=7108`, RR 目标=35%）
| 模型 | Precision(reject) | Recall(default) | Approval Bad Rate | Lift | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML (logistic_tabular) | 0.4154 | 0.4538 | 0.3098 | 1.1870 | 0.3826 |
| ML (logistic_text_fusion) | 0.4154 | 0.4337 | 0.3126 | 1.1868 | 0.3657 |
| ML (xgboost_tabular) | 0.4030 | 0.4378 | 0.3178 | 1.1513 | 0.3805 |
| DataAnalysis | 0.4175 | 0.4317 | 0.3120 | 1.1928 | 0.3622 |
| BERT-Embedding | 0.3859 | 0.4177 | 0.3284 | 1.1026 | 0.3790 |
| BERT-Finetune | 0.3895 | 0.4317 | 0.3253 | 1.1128 | 0.3882 |
| LLM 0.6B 两阶段微调 | 0.3653 | 0.3675 | 0.3420 | 1.0436 | 0.3523 |
| LLM 0.6B 单阶段微调 | 0.3587 | 0.6245 | 0.3369 | 1.0249 | 0.6097 |
| GPT-5.2 未微调 | 0.3462 | 0.8133 | 0.3690 | 0.9884 | 0.8228 |

解释：
1. 若要求“拒绝率接近 35%”的公平比较，主结论以前 7 条模型为准。  
2. 单阶段 LLM 与 GPT-5.2 的高召回对应高拒绝率，不满足公平主榜约束。

### 7.2 口径 B：全量样本（`n=123,202`, RR 目标=0.1533）
| 模型 | Precision(reject) | Recall(default) | Approval Bad Rate | Lift | 实际 Reject Rate |
|---|---:|---:|---:|---:|---:|
| ML (logistic_tabular) | 0.3069 | 0.2992 | 0.1263 | 2.0023 | 0.1494 |
| ML (logistic_text_fusion) | 0.2994 | 0.3050 | 0.1262 | 1.9532 | 0.1562 |
| ML (xgboost_tabular) | 0.3153 | 0.3095 | 0.1246 | 2.0568 | 0.1505 |
| DataAnalysis | 0.2849 | 0.2851 | 0.1294 | 1.8589 | 0.1534 |

解释：
1. 全量口径下 `xgboost_tabular` 综合最优。  
2. 说明模型排序与样本分布、目标拒绝率强相关。

### 7.3 关键发现
1. `H1` 得到支持：共享高风险口径下，结构化模型主判能力更稳健。  
2. `H2` 得到支持：部分 LLM 路线存在“高拒绝率换高召回”。  
3. `H3` 得到支持：全量口径模型排序变化明显（XGBoost 领先）。

## 8. 报告撰写与项目反思（Week 6）
### 8.1 与课程要求对齐的写作策略
本报告将“研究问题/方案/试点/分析/展示”做一一映射，避免只报告模型分数。

### 8.2 项目执行中的主要风险与处理
1. **公平性风险**：不同模型拒绝率偏移导致不可比。  
处理：单独拆分公平主榜与扩展榜。  
2. **数据口径风险**：子集结果难外推到全量业务。  
处理：增加全量口径评估。  
3. **可复现风险**：多脚本产物分散。  
处理：统一引用 `run_report.json` 与模块 `REPORT.md`。

## 9. 成果传播规划与最终展示（Week 7-8）
### 9.1 成果传播规划（Week 7）
计划两类输出：
1. **学术表达版**：问题定义、口径设计、关键对比结论、局限性。  
2. **业务沟通版**：拒绝率约束下的模型选择建议与上线风险。

建议传播载体：
1. 课程最终展示（主渠道）。  
2. 项目技术复盘文档（内部/同伴共享）。  
3. 可投稿方向的短论文提纲（后续延伸）。

### 9.2 最终展示结构（Week 8）
建议 12-15 页展示材料：
1. 研究背景与目标（2 页）  
2. 双口径设计与公平性原则（2 页）  
3. 数据筛选与切分（2 页）  
4. 主结果与扩展结果（4 页）  
5. 结论、局限与下一步（2-3 页）

现场演示可选内容：
1. 快速展示各模块 `run_report.json` 的关键字段。  
2. 演示“同一模型在不同 RR 目标下指标变化”的可解释性。  
3. 回答“为什么高召回不一定更优”的业务问题。

## 10. 结论与下一步
本项目完成了从研究问题定义到最终展示方案的端到端闭环，符合 DSCI 7000 顶点课程的项目型要求。核心结论：

1. 在受控拒绝率公平比较下，当前最稳健主线仍是结构化模型（DataAnalysis/ML）。  
2. 文本模型具备潜力，但需要先解决拒绝率校准与稳定性问题。  
3. 全量口径结果提示“子集结论不可直接外推”，因此双口径报告是必要设计。  
4. 项目已具备提交基础，可进一步增强展示图形化与跨时间稳健性实验。

下一步建议：
1. 为单阶段 LLM 与 GPT-5.2 增加严格 top-k 拒绝率控制，再做公平复评。  
2. 补充时间外推切分（按年份训练/测试）验证稳定性。  
3. 产出课程展示版幻灯片与 3 分钟/8 分钟双版本讲稿。

## 附录 A：关键证据文件
- 总报告：`REPORT.md`
- 总说明：`README.md`
- ML 结果：`ml_standalone/REPORT_ML.md`
- DataAnalysis 结果：`data_analysis_standalone/REPORT_DATA_ANALYSIS.md`
- BERT 结果：`bert_standalone/REPORT_BERT.md`
- LLM 结果：`llm_standalone/REPORT_LLM.md`
- 共享口径结果：`output/ml_standalone/run_report.json`
- 全量口径结果：`output/ml_standalone_full_actual_rr/run_report.json`
- 全量 DataAnalysis：`output/data_analysis_standalone_full_actual_rr/run_report.json`
- LLM 两阶段结果：`output/llm_two_stage_subset_full_06b/infer/stage2_action_predictions_full.json`
- LLM 单阶段结果：`output/llm_one_stage_subset_full_06b/infer/stage_action_direct_predictions_full.json`
- GPT-5.2 外部评估：`output/llm_two_stage_subset_full_06b/infer/gpt52_external_metrics.json`
